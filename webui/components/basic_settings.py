import traceback

import streamlit as st
import os
from app.config import config
from app.utils import utils
from loguru import logger
from app.services.llm.unified_service import UnifiedLLMService


def validate_api_key(api_key: str, provider: str) -> tuple[bool, str]:
    """éªŒè¯APIå¯†é’¥æ ¼å¼"""
    if not api_key or not api_key.strip():
        return False, f"{provider} APIå¯†é’¥ä¸èƒ½ä¸ºç©º"

    # åŸºæœ¬é•¿åº¦æ£€æŸ¥
    if len(api_key.strip()) < 10:
        return False, f"{provider} APIå¯†é’¥é•¿åº¦è¿‡çŸ­ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®"

    return True, ""


def get_api_key_strength_indicator(api_key: str) -> tuple[str, str]:
    """è·å–APIå¯†é’¥å¼ºåº¦æŒ‡ç¤ºå™¨
    
    Args:
        api_key: APIå¯†é’¥å­—ç¬¦ä¸²
        
    Returns:
        tuple[str, str]: (æŒ‡ç¤ºå™¨å›¾æ ‡, çŠ¶æ€æè¿°)
    """
    if not api_key or not api_key.strip():
        return "ğŸ”´", "æœªè®¾ç½®"
    
    api_key = api_key.strip()
    if len(api_key) < 10:
        return "ğŸŸ¡", "æ ¼å¼å¼‚å¸¸"
    
    # æ£€æŸ¥å¸¸è§APIå¯†é’¥æ ¼å¼æ¨¡å¼
    if api_key.startswith('sk-') and len(api_key) >= 20:
        return "ğŸŸ¢", "OpenAIæ ¼å¼"
    elif api_key.startswith('AIza') and len(api_key) >= 35:
        return "ğŸŸ¢", "Geminiæ ¼å¼"
    elif len(api_key) >= 32 and any(c.isupper() for c in api_key) and any(c.islower() for c in api_key) and any(c.isdigit() for c in api_key):
        return "ğŸŸ¢", "æ ¼å¼æ­£å¸¸"
    else:
        return "ğŸŸ¡", "å¾…éªŒè¯"


def validate_base_url(base_url: str, provider: str) -> tuple[bool, str]:
    """éªŒè¯Base URLæ ¼å¼"""
    if not base_url or not base_url.strip():
        return True, ""  # base_urlå¯ä»¥ä¸ºç©º

    base_url = base_url.strip()
    if not (base_url.startswith('http://') or base_url.startswith('https://')):
        return False, f"{provider} Base URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´"

    return True, ""


def validate_model_name(model_name: str, provider: str) -> tuple[bool, str]:
    """éªŒè¯æ¨¡å‹åç§°"""
    if not model_name or not model_name.strip():
        return False, f"{provider} æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º"

    return True, ""


def validate_litellm_model_name(model_name: str, model_type: str) -> tuple[bool, str]:
    """éªŒè¯ LiteLLM æ¨¡å‹åç§°æ ¼å¼
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œåº”ä¸º provider/model æ ¼å¼
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå¦‚"è§†é¢‘åˆ†æ"ã€"æ–‡æ¡ˆç”Ÿæˆ"ï¼‰
        
    Returns:
        (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯æ¶ˆæ¯)
    """
    if not model_name or not model_name.strip():
        return False, f"{model_type} æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º"
    
    model_name = model_name.strip()
    
    # LiteLLM æ¨èæ ¼å¼ï¼šprovider/modelï¼ˆå¦‚ gemini/gemini-2.0-flash-liteï¼‰
    # ä½†ä¹Ÿæ”¯æŒç›´æ¥çš„æ¨¡å‹åç§°ï¼ˆå¦‚ gpt-4oï¼ŒLiteLLM ä¼šè‡ªåŠ¨æ¨æ–­ providerï¼‰
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å« provider å‰ç¼€ï¼ˆæ¨èæ ¼å¼ï¼‰
    if "/" in model_name:
        parts = model_name.split("/")
        if len(parts) < 2 or not parts[0] or not parts[1]:
            return False, f"{model_type} æ¨¡å‹åç§°æ ¼å¼é”™è¯¯ã€‚æ¨èæ ¼å¼: provider/model ï¼ˆå¦‚ gemini/gemini-2.0-flash-liteï¼‰"
        
        # éªŒè¯ provider åç§°ï¼ˆåªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ï¼‰
        provider = parts[0]
        if not provider.replace("-", "").replace("_", "").isalnum():
            return False, f"{model_type} Provider åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦"
    else:
        # ç›´æ¥æ¨¡å‹åç§°ä¹Ÿæ˜¯æœ‰æ•ˆçš„ï¼ˆLiteLLM ä¼šè‡ªåŠ¨æ¨æ–­ï¼‰
        # ä½†ç»™å‡ºè­¦å‘Šå»ºè®®ä½¿ç”¨å®Œæ•´æ ¼å¼
        logger.debug(f"{model_type} æ¨¡å‹åç§°æœªåŒ…å« provider å‰ç¼€ï¼ŒLiteLLM å°†è‡ªåŠ¨æ¨æ–­")
    
    # åŸºæœ¬é•¿åº¦æ£€æŸ¥
    if len(model_name) < 3:
        return False, f"{model_type} æ¨¡å‹åç§°è¿‡çŸ­"
    
    if len(model_name) > 200:
        return False, f"{model_type} æ¨¡å‹åç§°è¿‡é•¿"
    
    return True, ""


def show_config_validation_errors(errors: list):
    """æ˜¾ç¤ºé…ç½®éªŒè¯é”™è¯¯"""
    if errors:
        for error in errors:
            st.error(error)


def render_basic_settings(tr):
    """æ¸²æŸ“åŸºç¡€è®¾ç½®é¢æ¿"""
    with st.expander(tr("Basic Settings"), expanded=False):
        config_panels = st.columns(3)
        left_config_panel = config_panels[0]
        middle_config_panel = config_panels[1]
        right_config_panel = config_panels[2]

        with left_config_panel:
            render_language_settings(tr)
            render_proxy_settings(tr)

        with middle_config_panel:
            render_vision_llm_settings(tr)  # è§†é¢‘åˆ†ææ¨¡å‹è®¾ç½®

        with right_config_panel:
            render_text_llm_settings(tr)  # æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è®¾ç½®


def render_language_settings(tr):
    st.subheader(tr("Proxy Settings"))

    """æ¸²æŸ“è¯­è¨€è®¾ç½®"""
    system_locale = utils.get_system_locale()
    i18n_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "i18n")
    locales = utils.load_locales(i18n_dir)

    display_languages = []
    selected_index = 0
    for i, code in enumerate(locales.keys()):
        display_languages.append(f"{code} - {locales[code].get('Language')}")
        if code == st.session_state.get('ui_language', system_locale):
            selected_index = i

    selected_language = st.selectbox(
        tr("Language"),
        options=display_languages,
        index=selected_index
    )

    if selected_language:
        code = selected_language.split(" - ")[0].strip()
        st.session_state['ui_language'] = code
        config.ui['language'] = code


def render_proxy_settings(tr):
    """æ¸²æŸ“ä»£ç†è®¾ç½®"""
    # è·å–å½“å‰ä»£ç†çŠ¶æ€
    proxy_enabled = config.proxy.get("enabled", False)
    proxy_url_http = config.proxy.get("http")
    proxy_url_https = config.proxy.get("https")

    # æ·»åŠ ä»£ç†å¼€å…³
    proxy_enabled = st.checkbox(tr("Enable Proxy"), value=proxy_enabled)
    
    # ä¿å­˜ä»£ç†å¼€å…³çŠ¶æ€
    # config.proxy["enabled"] = proxy_enabled

    # åªæœ‰åœ¨ä»£ç†å¯ç”¨æ—¶æ‰æ˜¾ç¤ºä»£ç†è®¾ç½®è¾“å…¥æ¡†
    if proxy_enabled:
        HTTP_PROXY = st.text_input(tr("HTTP_PROXY"), value=proxy_url_http)
        HTTPS_PROXY = st.text_input(tr("HTTPs_PROXY"), value=proxy_url_https)

        if HTTP_PROXY and HTTPS_PROXY:
            config.proxy["http"] = HTTP_PROXY
            config.proxy["https"] = HTTPS_PROXY
            os.environ["HTTP_PROXY"] = HTTP_PROXY
            os.environ["HTTPS_PROXY"] = HTTPS_PROXY
            # logger.debug(f"ä»£ç†å·²å¯ç”¨: {HTTP_PROXY}")
    else:
        # å½“ä»£ç†è¢«ç¦ç”¨æ—¶ï¼Œæ¸…é™¤ç¯å¢ƒå˜é‡å’Œé…ç½®
        os.environ.pop("HTTP_PROXY", None)
        os.environ.pop("HTTPS_PROXY", None)
        # config.proxy["http"] = ""
        # config.proxy["https"] = ""


def test_vision_model_connection(api_key, base_url, model_name, provider, tr):
    """æµ‹è¯•è§†è§‰æ¨¡å‹è¿æ¥

    Args:
        api_key: APIå¯†é’¥
        base_url: åŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        provider: æä¾›å•†åç§°

    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        str: æµ‹è¯•ç»“æœæ¶ˆæ¯
    """
    import requests
    logger.debug(f"å¤§æ¨¡å‹è¿é€šæ€§æµ‹è¯•: {base_url} æ¨¡å‹: {model_name} apikey: {api_key}")
    if provider.lower() == 'gemini':
        # åŸç”ŸGemini APIæµ‹è¯•
        try:
            # æ„å»ºè¯·æ±‚æ•°æ®
            request_data = {
                "contents": [{
                    "parts": [{"text": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}]
                }]
            }

            # æ„å»ºè¯·æ±‚URL
            api_base_url = base_url
            url = f"{api_base_url}/models/{model_name}:generateContent"
            # å‘é€è¯·æ±‚
            response = requests.post(
                url,
                json=request_data,
                headers={
                    "x-goog-api-key": api_key,
                    "Content-Type": "application/json"
                    },
                timeout=10
            )

            if response.status_code == 200:
                return True, tr("åŸç”ŸGeminiæ¨¡å‹è¿æ¥æˆåŠŸ")
            else:
                return False, f"{tr('åŸç”ŸGeminiæ¨¡å‹è¿æ¥å¤±è´¥')}: HTTP {response.status_code}"
        except Exception as e:
            return False, f"{tr('åŸç”ŸGeminiæ¨¡å‹è¿æ¥å¤±è´¥')}: {str(e)}"
    elif provider.lower() == 'gemini(openai)':
        # OpenAIå…¼å®¹çš„Geminiä»£ç†æµ‹è¯•
        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            test_url = f"{base_url.rstrip('/')}/chat/completions"
            test_data = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}
                ],
                "stream": False
            }

            response = requests.post(test_url, headers=headers, json=test_data, timeout=10)
            if response.status_code == 200:
                return True, tr("OpenAIå…¼å®¹Geminiä»£ç†è¿æ¥æˆåŠŸ")
            else:
                return False, f"{tr('OpenAIå…¼å®¹Geminiä»£ç†è¿æ¥å¤±è´¥')}: HTTP {response.status_code}"
        except Exception as e:
            return False, f"{tr('OpenAIå…¼å®¹Geminiä»£ç†è¿æ¥å¤±è´¥')}: {str(e)}"
    else:
        from openai import OpenAI
        try:
            client = OpenAI(
                api_key=api_key,
                base_url=base_url,
            )

            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": "You are a helpful assistant."}],
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": "https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241022/emyrja/dog_and_girl.jpeg"
                                },
                            },
                            {"type": "text", "text": "å›å¤æˆ‘ç½‘ç»œå¯ç”¨å³å¯"},
                        ],
                    },
                ],
            )
            if response and response.choices:
                return True, tr("QwenVL model is available")
            else:
                return False, tr("QwenVL model returned invalid response")

        except Exception as e:
            # logger.debug(api_key)
            # logger.debug(base_url)
            # logger.debug(model_name)
            return False, f"{tr('QwenVL model is not available')}: {str(e)}"




def test_litellm_vision_model(api_key: str, base_url: str, model_name: str, tr) -> tuple[bool, str]:
    """æµ‹è¯• LiteLLM è§†è§‰æ¨¡å‹è¿æ¥
    
    Args:
        api_key: API å¯†é’¥
        base_url: åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆLiteLLM æ ¼å¼ï¼šprovider/modelï¼‰
        tr: ç¿»è¯‘å‡½æ•°
        
    Returns:
        (è¿æ¥æ˜¯å¦æˆåŠŸ, æµ‹è¯•ç»“æœæ¶ˆæ¯)
    """
    try:
        import litellm
        import os
        import base64
        import io
        from PIL import Image
        
        logger.debug(f"LiteLLM è§†è§‰æ¨¡å‹è¿é€šæ€§æµ‹è¯•: model={model_name}, api_key={api_key[:10]}..., base_url={base_url}")
        
        # æå– provider åç§°
        provider = model_name.split("/")[0] if "/" in model_name else "unknown"
        
        # è®¾ç½® API key åˆ°ç¯å¢ƒå˜é‡
        env_key_mapping = {
            "gemini": "GEMINI_API_KEY",
            "google": "GEMINI_API_KEY",
            "openai": "OPENAI_API_KEY",
            "qwen": "QWEN_API_KEY",
            "dashscope": "DASHSCOPE_API_KEY",
            "siliconflow": "SILICONFLOW_API_KEY",
        }
        env_var = env_key_mapping.get(provider.lower(), f"{provider.upper()}_API_KEY")
        old_key = os.environ.get(env_var)
        os.environ[env_var] = api_key
        
        # SiliconFlow ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        test_model_name = model_name
        if provider.lower() == "siliconflow":
            # æ›¿æ¢ provider ä¸º openai
            if "/" in model_name:
                test_model_name = f"openai/{model_name.split('/', 1)[1]}"
            else:
                test_model_name = f"openai/{model_name}"
            
            # ç¡®ä¿è®¾ç½®äº† base_url
            if not base_url:
                base_url = "https://api.siliconflow.cn/v1"
            
            # è®¾ç½® OPENAI_API_KEY (SiliconFlow ä½¿ç”¨ OpenAI åè®®)
            os.environ["OPENAI_API_KEY"] = api_key
            os.environ["OPENAI_API_BASE"] = base_url
        
        try:
            # åˆ›å»ºæµ‹è¯•å›¾ç‰‡ï¼ˆ64x64 ç™½è‰²åƒç´ ï¼Œé¿å…æŸäº›æ¨¡å‹å¯¹æå°å›¾ç‰‡çš„é™åˆ¶ï¼‰
            test_image = Image.new('RGB', (64, 64), color='white')
            img_buffer = io.BytesIO()
            test_image.save(img_buffer, format='JPEG')
            img_bytes = img_buffer.getvalue()
            base64_image = base64.b64encode(img_bytes).decode('utf-8')
            
            # æ„å»ºæµ‹è¯•è¯·æ±‚
            messages = [{
                "role": "user",
                "content": [
                    {"type": "text", "text": "è¯·ç›´æ¥å›å¤'è¿æ¥æˆåŠŸ'"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }]
            
            # å‡†å¤‡å‚æ•°
            completion_kwargs = {
                "model": test_model_name,
                "messages": messages,
                "temperature": 0.1,
                "max_tokens": 50
            }
            
            if base_url:
                completion_kwargs["api_base"] = base_url
            
            # è°ƒç”¨ LiteLLMï¼ˆåŒæ­¥è°ƒç”¨ç”¨äºæµ‹è¯•ï¼‰
            response = litellm.completion(**completion_kwargs)
            
            if response and response.choices and len(response.choices) > 0:
                return True, f"LiteLLM è§†è§‰æ¨¡å‹è¿æ¥æˆåŠŸ ({model_name})"
            else:
                return False, f"LiteLLM è§†è§‰æ¨¡å‹è¿”å›ç©ºå“åº”"
                
        finally:
            # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
            if old_key:
                os.environ[env_var] = old_key
            else:
                os.environ.pop(env_var, None)
            
            # æ¸…ç†ä¸´æ—¶è®¾ç½®çš„ OpenAI ç¯å¢ƒå˜é‡
            if provider.lower() == "siliconflow":
                 os.environ.pop("OPENAI_API_KEY", None)
                 os.environ.pop("OPENAI_API_BASE", None)
                
    except Exception as e:
        error_msg = str(e)
        logger.error(f"LiteLLM è§†è§‰æ¨¡å‹æµ‹è¯•å¤±è´¥: {error_msg}")
        
        # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        if "authentication" in error_msg.lower() or "api_key" in error_msg.lower():
            return False, f"è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®"
        elif "not found" in error_msg.lower() or "404" in error_msg:
            return False, f"æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®"
        elif "rate limit" in error_msg.lower():
            return False, f"è¶…å‡ºé€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
        else:
            return False, f"è¿æ¥å¤±è´¥: {error_msg}"


def test_litellm_text_model(api_key: str, base_url: str, model_name: str, tr) -> tuple[bool, str]:
    """æµ‹è¯• LiteLLM æ–‡æœ¬æ¨¡å‹è¿æ¥
    
    Args:
        api_key: API å¯†é’¥
        base_url: åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆLiteLLM æ ¼å¼ï¼šprovider/modelï¼‰
        tr: ç¿»è¯‘å‡½æ•°
        
    Returns:
        (è¿æ¥æ˜¯å¦æˆåŠŸ, æµ‹è¯•ç»“æœæ¶ˆæ¯)
    """
    try:
        import litellm
        import os
        
        logger.debug(f"LiteLLM æ–‡æœ¬æ¨¡å‹è¿é€šæ€§æµ‹è¯•: model={model_name}, api_key={api_key[:10]}..., base_url={base_url}")
        
        # æå– provider åç§°
        provider = model_name.split("/")[0] if "/" in model_name else "unknown"
        
        # è®¾ç½® API key åˆ°ç¯å¢ƒå˜é‡
        env_key_mapping = {
            "gemini": "GEMINI_API_KEY",
            "google": "GEMINI_API_KEY",
            "openai": "OPENAI_API_KEY",
            "qwen": "QWEN_API_KEY",
            "dashscope": "DASHSCOPE_API_KEY",
            "siliconflow": "SILICONFLOW_API_KEY",
            "deepseek": "DEEPSEEK_API_KEY",
            "moonshot": "MOONSHOT_API_KEY",
        }
        env_var = env_key_mapping.get(provider.lower(), f"{provider.upper()}_API_KEY")
        old_key = os.environ.get(env_var)
        os.environ[env_var] = api_key
        
        # SiliconFlow ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        test_model_name = model_name
        if provider.lower() == "siliconflow":
            # æ›¿æ¢ provider ä¸º openai
            if "/" in model_name:
                test_model_name = f"openai/{model_name.split('/', 1)[1]}"
            else:
                test_model_name = f"openai/{model_name}"
            
            # ç¡®ä¿è®¾ç½®äº† base_url
            if not base_url:
                base_url = "https://api.siliconflow.cn/v1"
            
            # è®¾ç½® OPENAI_API_KEY (SiliconFlow ä½¿ç”¨ OpenAI åè®®)
            os.environ["OPENAI_API_KEY"] = api_key
            os.environ["OPENAI_API_BASE"] = base_url
        
        try:
            # æ„å»ºæµ‹è¯•è¯·æ±‚
            messages = [
                {"role": "user", "content": "è¯·ç›´æ¥å›å¤'è¿æ¥æˆåŠŸ'"}
            ]
            
            # å‡†å¤‡å‚æ•°
            completion_kwargs = {
                "model": test_model_name,
                "messages": messages,
                "temperature": 0.1,
                "max_tokens": 20
            }
            
            if base_url:
                completion_kwargs["api_base"] = base_url
            
            # è°ƒç”¨ LiteLLMï¼ˆåŒæ­¥è°ƒç”¨ç”¨äºæµ‹è¯•ï¼‰
            response = litellm.completion(**completion_kwargs)
            
            if response and response.choices and len(response.choices) > 0:
                return True, f"LiteLLM æ–‡æœ¬æ¨¡å‹è¿æ¥æˆåŠŸ ({model_name})"
            else:
                return False, f"LiteLLM æ–‡æœ¬æ¨¡å‹è¿”å›ç©ºå“åº”"
                
        finally:
            # æ¢å¤åŸå§‹ç¯å¢ƒå˜é‡
            if old_key:
                os.environ[env_var] = old_key
            else:
                os.environ.pop(env_var, None)
            
            # æ¸…ç†ä¸´æ—¶è®¾ç½®çš„ OpenAI ç¯å¢ƒå˜é‡
            if provider.lower() == "siliconflow":
                 os.environ.pop("OPENAI_API_KEY", None)
                 os.environ.pop("OPENAI_API_BASE", None)
                
    except Exception as e:
        error_msg = str(e)
        logger.error(f"LiteLLM æ–‡æœ¬æ¨¡å‹æµ‹è¯•å¤±è´¥: {error_msg}")
        
        # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        if "authentication" in error_msg.lower() or "api_key" in error_msg.lower():
            return False, f"è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®"
        elif "not found" in error_msg.lower() or "404" in error_msg:
            return False, f"æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®"
        elif "rate limit" in error_msg.lower():
            return False, f"è¶…å‡ºé€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
        else:
            return False, f"è¿æ¥å¤±è´¥: {error_msg}"

def render_vision_llm_settings(tr):
    """æ¸²æŸ“è§†é¢‘åˆ†ææ¨¡å‹è®¾ç½®ï¼ˆLiteLLM ç»Ÿä¸€é…ç½®ï¼‰"""
    st.subheader(tr("Vision Model Settings"))

    # å›ºå®šä½¿ç”¨ LiteLLM æä¾›å•†
    config.app["vision_llm_provider"] = "litellm"

    # è·å–å·²ä¿å­˜çš„ LiteLLM é…ç½®
    full_vision_model_name = config.app.get("vision_litellm_model_name", "gemini/gemini-2.0-flash-lite")
    vision_api_key = config.app.get("vision_litellm_api_key", "")
    vision_base_url = config.app.get("vision_litellm_base_url", "")

    # è§£æ provider å’Œ model
    default_provider = "gemini"
    default_model = "gemini-2.0-flash-lite"
    
    if "/" in full_vision_model_name:
        parts = full_vision_model_name.split("/", 1)
        current_provider = parts[0]
        current_model = parts[1]
    else:
        current_provider = default_provider
        current_model = full_vision_model_name

    # å®šä¹‰æ”¯æŒçš„ provider åˆ—è¡¨
    LITELLM_PROVIDERS = [
        "openai", "gemini", "deepseek", "qwen", "siliconflow", "moonshot", 
        "anthropic", "azure", "ollama", "vertex_ai", "mistral", "codestral", 
        "volcengine", "groq", "cohere", "together_ai", "fireworks_ai", 
        "openrouter", "replicate", "huggingface", "xai", "deepgram", "vllm", 
        "bedrock", "cloudflare"
    ]
    
    # å¦‚æœå½“å‰ provider ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ·»åŠ åˆ°åˆ—è¡¨å¤´éƒ¨
    if current_provider not in LITELLM_PROVIDERS:
        LITELLM_PROVIDERS.insert(0, current_provider)

    # æ¸²æŸ“é…ç½®è¾“å…¥æ¡†
    col1, col2 = st.columns([1, 2])
    with col1:
        selected_provider = st.selectbox(
            tr("Vision Model Provider"),
            options=LITELLM_PROVIDERS,
            index=LITELLM_PROVIDERS.index(current_provider) if current_provider in LITELLM_PROVIDERS else 0,
            key="vision_provider_select"
        )
    
    with col2:
        model_name_input = st.text_input(
            tr("Vision Model Name"),
            value=current_model,
            help="è¾“å…¥æ¨¡å‹åç§°ï¼ˆä¸åŒ…å« provider å‰ç¼€ï¼‰\n\n"
                 "å¸¸ç”¨ç¤ºä¾‹:\n"
                 "â€¢ gemini-2.0-flash-lite\n"
                 "â€¢ gpt-4o\n"
                 "â€¢ qwen-vl-max\n"
                 "â€¢ Qwen/Qwen2.5-VL-32B-Instruct (SiliconFlow)\n\n"
                 "æ”¯æŒ 100+ providersï¼Œè¯¦è§: https://docs.litellm.ai/docs/providers",
            key="vision_model_input"
        )

    # ç»„åˆå®Œæ•´çš„æ¨¡å‹åç§°
    st_vision_model_name = f"{selected_provider}/{model_name_input}" if selected_provider and model_name_input else ""

    # è·å–APIå¯†é’¥å¼ºåº¦æŒ‡ç¤ºå™¨
    vision_indicator, vision_status = get_api_key_strength_indicator(vision_api_key)
    
    # ä½¿ç”¨åˆ—å¸ƒå±€æ¥å¹¶æ’æ˜¾ç¤ºè¾“å…¥æ¡†å’ŒæŒ‡ç¤ºå™¨
    col1, col2 = st.columns([4, 1])
    with col1:
        st_vision_api_key = st.text_input(
            tr("Vision API Key"),
            value=vision_api_key,
            type="password",
            help="å¯¹åº” provider çš„ API å¯†é’¥\n\n"
                 "è·å–åœ°å€:\n"
                 "â€¢ Gemini: https://makersuite.google.com/app/apikey\n"
                 "â€¢ OpenAI: https://platform.openai.com/api-keys\n"
                 "â€¢ Qwen: https://bailian.console.aliyun.com/\n"
                 "â€¢ SiliconFlow: https://cloud.siliconflow.cn/account/ak"
        )
    with col2:
        # æ˜¾ç¤ºAPIå¯†é’¥å¼ºåº¦æŒ‡ç¤ºå™¨
        st.metric("APIçŠ¶æ€", vision_indicator, help=f"APIå¯†é’¥çŠ¶æ€: {vision_status}")
        
    # å®æ—¶æ›´æ–°æŒ‡ç¤ºå™¨ï¼ˆå½“APIå¯†é’¥æ”¹å˜æ—¶ï¼‰
    if st_vision_api_key != vision_api_key:
        new_indicator, new_status = get_api_key_strength_indicator(st_vision_api_key)
        if new_indicator != vision_indicator:
            st.rerun()

    st_vision_base_url = st.text_input(
        tr("Vision Base URL"),
        value=vision_base_url,
        help="è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰æ‰¾ä¸åˆ°ä¾›åº”å•†æ‰éœ€è¦å¡«è‡ªå®šä¹‰ url"
    )

    # æ·»åŠ æµ‹è¯•è¿æ¥æŒ‰é’®
    if st.button(tr("Test Connection"), key="test_vision_connection"):
        test_errors = []
        if not st_vision_api_key:
            test_errors.append("è¯·å…ˆè¾“å…¥ API å¯†é’¥")
        if not model_name_input:
            test_errors.append("è¯·å…ˆè¾“å…¥æ¨¡å‹åç§°")

        if test_errors:
            for error in test_errors:
                st.error(error)
        else:
            with st.spinner(tr("Testing connection...")):
                try:
                    success, message = test_litellm_vision_model(
                        api_key=st_vision_api_key,
                        base_url=st_vision_base_url,
                        model_name=st_vision_model_name,
                        tr=tr
                    )

                    if success:
                        st.success(message)
                    else:
                        st.error(message)
                except Exception as e:
                    st.error(f"æµ‹è¯•è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    logger.error(f"LiteLLM è§†é¢‘åˆ†ææ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")

    # éªŒè¯å’Œä¿å­˜é…ç½®
    validation_errors = []
    config_changed = False

    # éªŒè¯æ¨¡å‹åç§°
    if st_vision_model_name:
        # è¿™é‡Œçš„éªŒè¯é€»è¾‘å¯èƒ½éœ€è¦å¾®è°ƒï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æ˜¯è‡ªåŠ¨ç»„åˆçš„
        is_valid, error_msg = validate_litellm_model_name(st_vision_model_name, "è§†é¢‘åˆ†æ")
        if is_valid:
            config.app["vision_litellm_model_name"] = st_vision_model_name
            st.session_state["vision_litellm_model_name"] = st_vision_model_name
            config_changed = True
        else:
            validation_errors.append(error_msg)

    # éªŒè¯ API å¯†é’¥
    if st_vision_api_key:
        is_valid, error_msg = validate_api_key(st_vision_api_key, "è§†é¢‘åˆ†æ")
        if is_valid:
            config.app["vision_litellm_api_key"] = st_vision_api_key
            st.session_state["vision_litellm_api_key"] = st_vision_api_key
            config_changed = True
        else:
            validation_errors.append(error_msg)

    # éªŒè¯ Base URLï¼ˆå¯é€‰ï¼‰
    if st_vision_base_url:
        is_valid, error_msg = validate_base_url(st_vision_base_url, "è§†é¢‘åˆ†æ")
        if is_valid:
            config.app["vision_litellm_base_url"] = st_vision_base_url
            st.session_state["vision_litellm_base_url"] = st_vision_base_url
            config_changed = True
        else:
            validation_errors.append(error_msg)

    # æ˜¾ç¤ºéªŒè¯é”™è¯¯
    show_config_validation_errors(validation_errors)

    # ä¿å­˜é…ç½®
    if config_changed and not validation_errors:
        try:
            config.save_config()
            # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡ä½¿ç”¨æ–°é…ç½®
            UnifiedLLMService.clear_cache()
            if st_vision_api_key or st_vision_base_url or st_vision_model_name:
                st.success(f"è§†é¢‘åˆ†ææ¨¡å‹é…ç½®å·²ä¿å­˜ï¼ˆLiteLLMï¼‰")
        except Exception as e:
            st.error(f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")
            logger.error(f"ä¿å­˜è§†é¢‘åˆ†æé…ç½®å¤±è´¥: {str(e)}")


def test_text_model_connection(api_key, base_url, model_name, provider, tr):
    """æµ‹è¯•æ–‡æœ¬æ¨¡å‹è¿æ¥
    
    Args:
        api_key: APIå¯†é’¥
        base_url: åŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        provider: æä¾›å•†åç§°
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        str: æµ‹è¯•ç»“æœæ¶ˆæ¯
    """
    import requests
    logger.debug(f"å¤§æ¨¡å‹è¿é€šæ€§æµ‹è¯•: {base_url} æ¨¡å‹: {model_name} apikey: {api_key}")

    try:
        # æ„å»ºç»Ÿä¸€çš„æµ‹è¯•è¯·æ±‚ï¼ˆéµå¾ªOpenAIæ ¼å¼ï¼‰
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        # ç‰¹æ®Šå¤„ç†Gemini
        if provider.lower() == 'gemini':
            # åŸç”ŸGemini APIæµ‹è¯•
            try:
                # æ„å»ºè¯·æ±‚æ•°æ®
                request_data = {
                    "contents": [{
                        "parts": [{"text": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}]
                    }]
                }

                # æ„å»ºè¯·æ±‚URL
                api_base_url = base_url
                url = f"{api_base_url}/models/{model_name}:generateContent"

                # å‘é€è¯·æ±‚
                response = requests.post(
                    url,
                    json=request_data,
                    headers={
                        "x-goog-api-key": api_key,
                        "Content-Type": "application/json"
                    },
                    timeout=10
                )

                if response.status_code == 200:
                    return True, tr("åŸç”ŸGeminiæ¨¡å‹è¿æ¥æˆåŠŸ")
                else:
                    return False, f"{tr('åŸç”ŸGeminiæ¨¡å‹è¿æ¥å¤±è´¥')}: HTTP {response.status_code}"
            except Exception as e:
                return False, f"{tr('åŸç”ŸGeminiæ¨¡å‹è¿æ¥å¤±è´¥')}: {str(e)}"

        elif provider.lower() == 'gemini(openai)':
            # OpenAIå…¼å®¹çš„Geminiä»£ç†æµ‹è¯•
            test_url = f"{base_url.rstrip('/')}/chat/completions"
            test_data = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}
                ],
                "stream": False
            }

            response = requests.post(test_url, headers=headers, json=test_data, timeout=10)
            if response.status_code == 200:
                return True, tr("OpenAIå…¼å®¹Geminiä»£ç†è¿æ¥æˆåŠŸ")
            else:
                return False, f"{tr('OpenAIå…¼å®¹Geminiä»£ç†è¿æ¥å¤±è´¥')}: HTTP {response.status_code}"
        else:
            test_url = f"{base_url.rstrip('/')}/chat/completions"

            # æ„å»ºæµ‹è¯•æ¶ˆæ¯
            test_data = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": "ç›´æ¥å›å¤æˆ‘æ–‡æœ¬'å½“å‰ç½‘ç»œå¯ç”¨'"}
                ],
                "stream": False
            }

            # å‘é€æµ‹è¯•è¯·æ±‚
            response = requests.post(
                test_url,
                headers=headers,
                json=test_data,
            )
            # logger.debug(model_name)
            # logger.debug(api_key)
            # logger.debug(test_url)
            if response.status_code == 200:
                return True, tr("Text model is available")
            else:
                return False, f"{tr('Text model is not available')}: HTTP {response.status_code}"
            
    except Exception as e:
        logger.error(traceback.format_exc())
        return False, f"{tr('Connection failed')}: {str(e)}"


def render_text_llm_settings(tr):
    """æ¸²æŸ“æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è®¾ç½®ï¼ˆLiteLLM ç»Ÿä¸€é…ç½®ï¼‰"""
    st.subheader(tr("Text Generation Model Settings"))

    # å›ºå®šä½¿ç”¨ LiteLLM æä¾›å•†
    config.app["text_llm_provider"] = "litellm"

    # è·å–å·²ä¿å­˜çš„ LiteLLM é…ç½®
    full_text_model_name = config.app.get("text_litellm_model_name", "deepseek/deepseek-chat")
    text_api_key = config.app.get("text_litellm_api_key", "")
    text_base_url = config.app.get("text_litellm_base_url", "")

    # è§£æ provider å’Œ model
    default_provider = "deepseek"
    default_model = "deepseek-chat"
    
    if "/" in full_text_model_name:
        parts = full_text_model_name.split("/", 1)
        current_provider = parts[0]
        current_model = parts[1]
    else:
        current_provider = default_provider
        current_model = full_text_model_name

    # å®šä¹‰æ”¯æŒçš„ provider åˆ—è¡¨
    LITELLM_PROVIDERS = [
        "openai", "gemini", "deepseek", "qwen", "siliconflow", "moonshot", 
        "anthropic", "azure", "ollama", "vertex_ai", "mistral", "codestral", 
        "volcengine", "groq", "cohere", "together_ai", "fireworks_ai", 
        "openrouter", "replicate", "huggingface", "xai", "deepgram", "vllm", 
        "bedrock", "cloudflare"
    ]
    
    # å¦‚æœå½“å‰ provider ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ·»åŠ åˆ°åˆ—è¡¨å¤´éƒ¨
    if current_provider not in LITELLM_PROVIDERS:
        LITELLM_PROVIDERS.insert(0, current_provider)

    # æ¸²æŸ“é…ç½®è¾“å…¥æ¡†
    col1, col2 = st.columns([1, 2])
    with col1:
        selected_provider = st.selectbox(
            tr("Text Model Provider"),
            options=LITELLM_PROVIDERS,
            index=LITELLM_PROVIDERS.index(current_provider) if current_provider in LITELLM_PROVIDERS else 0,
            key="text_provider_select"
        )
    
    with col2:
        model_name_input = st.text_input(
            tr("Text Model Name"),
            value=current_model,
            help="è¾“å…¥æ¨¡å‹åç§°ï¼ˆä¸åŒ…å« provider å‰ç¼€ï¼‰\n\n"
                 "å¸¸ç”¨ç¤ºä¾‹:\n"
                 "â€¢ deepseek-chat\n"
                 "â€¢ gpt-4o\n"
                 "â€¢ gemini-2.0-flash\n"
                 "â€¢ deepseek-ai/DeepSeek-R1 (SiliconFlow)\n\n"
                 "æ”¯æŒ 100+ providersï¼Œè¯¦è§: https://docs.litellm.ai/docs/providers",
            key="text_model_input"
        )

    # ç»„åˆå®Œæ•´çš„æ¨¡å‹åç§°
    st_text_model_name = f"{selected_provider}/{model_name_input}" if selected_provider and model_name_input else ""

    # è·å–APIå¯†é’¥å¼ºåº¦æŒ‡ç¤ºå™¨
    text_indicator, text_status = get_api_key_strength_indicator(text_api_key)
    
    # ä½¿ç”¨åˆ—å¸ƒå±€æ¥å¹¶æ’æ˜¾ç¤ºè¾“å…¥æ¡†å’ŒæŒ‡ç¤ºå™¨
    col1, col2 = st.columns([4, 1])
    with col1:
        st_text_api_key = st.text_input(
            tr("Text API Key"),
            value=text_api_key,
            type="password",
            help="å¯¹åº” provider çš„ API å¯†é’¥\n\n"
                 "è·å–åœ°å€:\n"
                 "â€¢ DeepSeek: https://platform.deepseek.com/api_keys\n"
                 "â€¢ Gemini: https://makersuite.google.com/app/apikey\n"
                 "â€¢ OpenAI: https://platform.openai.com/api-keys\n"
                 "â€¢ Qwen: https://bailian.console.aliyun.com/\n"
                 "â€¢ SiliconFlow: https://cloud.siliconflow.cn/account/ak\n"
                 "â€¢ Moonshot: https://platform.moonshot.cn/console/api-keys"
        )
    with col2:
        # æ˜¾ç¤ºAPIå¯†é’¥å¼ºåº¦æŒ‡ç¤ºå™¨
        st.metric("APIçŠ¶æ€", text_indicator, help=f"APIå¯†é’¥çŠ¶æ€: {text_status}")
        
    # å®æ—¶æ›´æ–°æŒ‡ç¤ºå™¨ï¼ˆå½“APIå¯†é’¥æ”¹å˜æ—¶ï¼‰
    if st_text_api_key != text_api_key:
        new_indicator, new_status = get_api_key_strength_indicator(st_text_api_key)
        if new_indicator != text_indicator:
            st.rerun()

    st_text_base_url = st.text_input(
        tr("Text Base URL"),
        value=text_base_url,
        help="è‡ªå®šä¹‰ API ç«¯ç‚¹ï¼ˆå¯é€‰ï¼‰æ‰¾ä¸åˆ°ä¾›åº”å•†æ‰éœ€è¦å¡«è‡ªå®šä¹‰ url"
    )

    # æ·»åŠ æµ‹è¯•è¿æ¥æŒ‰é’®
    if st.button(tr("Test Connection"), key="test_text_connection"):
        test_errors = []
        if not st_text_api_key:
            test_errors.append("è¯·å…ˆè¾“å…¥ API å¯†é’¥")
        if not model_name_input:
            test_errors.append("è¯·å…ˆè¾“å…¥æ¨¡å‹åç§°")

        if test_errors:
            for error in test_errors:
                st.error(error)
        else:
            with st.spinner(tr("Testing connection...")):
                try:
                    success, message = test_litellm_text_model(
                        api_key=st_text_api_key,
                        base_url=st_text_base_url,
                        model_name=st_text_model_name,
                        tr=tr
                    )

                    if success:
                        st.success(message)
                    else:
                        st.error(message)
                except Exception as e:
                    st.error(f"æµ‹è¯•è¿æ¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    logger.error(f"LiteLLM æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")

    # éªŒè¯å’Œä¿å­˜é…ç½®
    text_validation_errors = []
    text_config_changed = False

    # éªŒè¯æ¨¡å‹åç§°
    if st_text_model_name:
        is_valid, error_msg = validate_litellm_model_name(st_text_model_name, "æ–‡æ¡ˆç”Ÿæˆ")
        if is_valid:
            config.app["text_litellm_model_name"] = st_text_model_name
            st.session_state["text_litellm_model_name"] = st_text_model_name
            text_config_changed = True
        else:
            text_validation_errors.append(error_msg)

    # éªŒè¯ API å¯†é’¥
    if st_text_api_key:
        is_valid, error_msg = validate_api_key(st_text_api_key, "æ–‡æ¡ˆç”Ÿæˆ")
        if is_valid:
            config.app["text_litellm_api_key"] = st_text_api_key
            st.session_state["text_litellm_api_key"] = st_text_api_key
            text_config_changed = True
        else:
            text_validation_errors.append(error_msg)

    # éªŒè¯ Base URLï¼ˆå¯é€‰ï¼‰
    if st_text_base_url:
        is_valid, error_msg = validate_base_url(st_text_base_url, "æ–‡æ¡ˆç”Ÿæˆ")
        if is_valid:
            config.app["text_litellm_base_url"] = st_text_base_url
            st.session_state["text_litellm_base_url"] = st_text_base_url
            text_config_changed = True
        else:
            text_validation_errors.append(error_msg)

    # æ˜¾ç¤ºéªŒè¯é”™è¯¯
    show_config_validation_errors(text_validation_errors)

    # ä¿å­˜é…ç½®
    if text_config_changed and not text_validation_errors:
        try:
            config.save_config()
            # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡ä½¿ç”¨æ–°é…ç½®
            UnifiedLLMService.clear_cache()
            if st_text_api_key or st_text_base_url or st_text_model_name:
                st.success(f"æ–‡æ¡ˆç”Ÿæˆæ¨¡å‹é…ç½®å·²ä¿å­˜ï¼ˆLiteLLMï¼‰")
        except Exception as e:
            st.error(f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")
            logger.error(f"ä¿å­˜æ–‡æ¡ˆç”Ÿæˆé…ç½®å¤±è´¥: {str(e)}")

    # # Cloudflare ç‰¹æ®Šé…ç½®
    # if text_provider == 'cloudflare':
    #     st_account_id = st.text_input(
    #         tr("Account ID"),
    #         value=config.app.get(f"text_{text_provider}_account_id", "")
    #     )
    #     if st_account_id:
    #         config.app[f"text_{text_provider}_account_id"] = st_account_id
