import streamlit as st
import os
from uuid import uuid4
from app.config import config
from app.services import voice
from app.models.schema import AudioVolumeDefaults
from app.utils import utils
from webui.utils.cache import get_songs_cache


def get_soulvoice_voices():
    """è·å– SoulVoice è¯­éŸ³åˆ—è¡¨"""
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº† SoulVoice API key
    api_key = config.soulvoice.get("api_key", "")
    if not api_key:
        return []

    # åªè¿”å›ä¸€ä¸ª SoulVoice é€‰é¡¹ï¼ŒéŸ³è‰²é€šè¿‡è¾“å…¥æ¡†è‡ªå®šä¹‰
    return ["soulvoice:custom"]


def get_tts_engine_options():
    """è·å–TTSå¼•æ“é€‰é¡¹"""
    return {
        "edge_tts": "Edge TTS",
        "azure_speech": "Azure Speech Services",
        "tencent_tts": "è…¾è®¯äº‘ TTS",
        "qwen3_tts": "é€šä¹‰åƒé—® Qwen3 TTS",
        "indextts2": "IndexTTS2 è¯­éŸ³å…‹éš†"
    }


def get_tts_engine_descriptions():
    """è·å–TTSå¼•æ“è¯¦ç»†æè¿°"""
    return {
        "edge_tts": {
            "title": "Edge TTS",
            "features": "å®Œå…¨å…è´¹ï¼Œä½†æœåŠ¡ç¨³å®šæ€§ä¸€èˆ¬ï¼Œä¸æ”¯æŒè¯­éŸ³å…‹éš†åŠŸèƒ½",
            "use_case": "æµ‹è¯•å’Œè½»é‡çº§ä½¿ç”¨",
            "registration": None
        },
        "azure_speech": {
            "title": "Azure Speech Services",
            "features": "æä¾›ä¸€å®šå…è´¹é¢åº¦ï¼Œè¶…å‡ºåæŒ‰é‡ä»˜è´¹ï¼Œéœ€è¦ç»‘å®šæµ·å¤–ä¿¡ç”¨å¡",
            "use_case": "ä¼ä¸šçº§åº”ç”¨ï¼Œéœ€è¦ç¨³å®šæœåŠ¡",
            "registration": "https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices"
        },
        "tencent_tts": {
            "title": "è…¾è®¯äº‘ TTS",
            "features": "æä¾›å…è´¹é¢åº¦ï¼ŒéŸ³è´¨ä¼˜ç§€ï¼Œæ”¯æŒå¤šç§éŸ³è‰²ï¼Œå›½å†…è®¿é—®é€Ÿåº¦å¿«",
            "use_case": "ä¸ªäººå’Œä¼ä¸šç”¨æˆ·ï¼Œéœ€è¦ç¨³å®šçš„ä¸­æ–‡è¯­éŸ³åˆæˆ",
            "registration": "https://console.cloud.tencent.com/tts"
        },
        "qwen3_tts": {
            "title": "é€šä¹‰åƒé—® Qwen3 TTS",
            "features": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®è¯­éŸ³åˆæˆï¼ŒéŸ³è´¨ä¼˜ç§€ï¼Œæ”¯æŒå¤šç§éŸ³è‰²",
            "use_case": "éœ€è¦é«˜è´¨é‡ä¸­æ–‡è¯­éŸ³åˆæˆçš„ç”¨æˆ·",
            "registration": "https://dashscope.aliyuncs.com/"
        },
        "indextts2": {
            "title": "IndexTTS2 è¯­éŸ³å…‹éš†",
            "features": "é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ï¼Œä¸Šä¼ å‚è€ƒéŸ³é¢‘å³å¯åˆæˆç›¸åŒéŸ³è‰²çš„è¯­éŸ³ï¼Œéœ€è¦æœ¬åœ°æˆ–ç§æœ‰éƒ¨ç½²",
            "use_case": "ä¸‹è½½åœ°å€ï¼šhttps://pan.quark.cn/s/0767c9bcefd5",
            "registration": None
        }
    }


def is_valid_azure_voice_name(voice_name: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„AzureéŸ³è‰²åç§°æ ¼å¼"""
    if not voice_name or not isinstance(voice_name, str):
        return False

    voice_name = voice_name.strip()

    # AzureéŸ³è‰²åç§°é€šå¸¸æ ¼å¼ä¸º: [è¯­è¨€]-[åœ°åŒº]-[åç§°]Neural
    # ä¾‹å¦‚: zh-CN-YunzeNeural, en-US-AvaMultilingualNeural
    import re
    pattern = r'^[a-z]{2}-[A-Z]{2}-\w+Neural$'
    return bool(re.match(pattern, voice_name))


def render_audio_panel(tr):
    """æ¸²æŸ“éŸ³é¢‘è®¾ç½®é¢æ¿"""
    with st.container(border=True):
        st.write(tr("Audio Settings"))

        # æ¸²æŸ“TTSè®¾ç½®
        render_tts_settings(tr)

        # æ¸²æŸ“èƒŒæ™¯éŸ³ä¹è®¾ç½®
        render_bgm_settings(tr)


def render_tts_settings(tr):
    """æ¸²æŸ“TTS(æ–‡æœ¬è½¬è¯­éŸ³)è®¾ç½®"""

    # 1. TTSå¼•æ“é€‰æ‹©å™¨
    # st.subheader("ğŸ¤ TTSå¼•æ“é€‰æ‹©")

    engine_options = get_tts_engine_options()
    engine_descriptions = get_tts_engine_descriptions()

    # è·å–ä¿å­˜çš„TTSå¼•æ“è®¾ç½®
    saved_tts_engine = config.ui.get("tts_engine", "edge_tts")

    # ç¡®ä¿ä¿å­˜çš„å¼•æ“åœ¨å¯ç”¨é€‰é¡¹ä¸­
    if saved_tts_engine not in engine_options:
        saved_tts_engine = "edge_tts"

    # TTSå¼•æ“é€‰æ‹©ä¸‹æ‹‰æ¡†
    selected_engine = st.selectbox(
        "é€‰æ‹©TTSå¼•æ“",
        options=list(engine_options.keys()),
        format_func=lambda x: engine_options[x],
        index=list(engine_options.keys()).index(saved_tts_engine),
        help="é€‰æ‹©æ‚¨è¦ä½¿ç”¨çš„æ–‡æœ¬è½¬è¯­éŸ³å¼•æ“"
    )

    # ä¿å­˜TTSå¼•æ“é€‰æ‹©
    config.ui["tts_engine"] = selected_engine
    st.session_state['tts_engine'] = selected_engine

    # 2. æ˜¾ç¤ºå¼•æ“è¯¦ç»†è¯´æ˜
    if selected_engine in engine_descriptions:
        desc = engine_descriptions[selected_engine]

        with st.expander(f"ğŸ“‹ {desc['title']} è¯¦ç»†è¯´æ˜", expanded=True):
            st.markdown(f"**ç‰¹ç‚¹ï¼š** {desc['features']}")
            st.markdown(f"**é€‚ç”¨åœºæ™¯ï¼š** {desc['use_case']}")

            if desc['registration']:
                st.markdown(f"**æ³¨å†Œåœ°å€ï¼š** [{desc['registration']}]({desc['registration']})")

    # 3. æ ¹æ®é€‰æ‹©çš„å¼•æ“æ¸²æŸ“å¯¹åº”çš„é…ç½®ç•Œé¢
    # st.subheader("âš™ï¸ å¼•æ“é…ç½®")

    if selected_engine == "edge_tts":
        render_edge_tts_settings(tr)
    elif selected_engine == "azure_speech":
        render_azure_speech_settings(tr)
    elif selected_engine == "soulvoice":
        render_soulvoice_engine_settings(tr)
    elif selected_engine == "tencent_tts":
        render_tencent_tts_settings(tr)
    elif selected_engine == "qwen3_tts":
        render_qwen3_tts_settings(tr)
    elif selected_engine == "indextts2":
        render_indextts2_tts_settings(tr)

    # 4. è¯•å¬åŠŸèƒ½
    render_voice_preview_new(tr, selected_engine)


def render_edge_tts_settings(tr):
    """æ¸²æŸ“ Edge TTS å¼•æ“è®¾ç½®"""
    # è·å–æ”¯æŒçš„è¯­éŸ³åˆ—è¡¨
    support_locales = ["zh-CN", "en-US"]
    all_voices = voice.get_all_azure_voices(filter_locals=support_locales)

    # åªä¿ç•™æ ‡å‡†ç‰ˆæœ¬çš„è¯­éŸ³ï¼ˆEdge TTSä¸“ç”¨ï¼Œä¸åŒ…å«V2ï¼‰
    edge_voices = [v for v in all_voices if "-V2" not in v]

    # åˆ›å»ºå‹å¥½çš„æ˜¾ç¤ºåç§°
    friendly_names = {}
    for v in edge_voices:
        friendly_names[v] = v.replace("Female", tr("Female")).replace("Male", tr("Male")).replace("Neural", "")

    # è·å–ä¿å­˜çš„è¯­éŸ³è®¾ç½®
    saved_voice_name = config.ui.get("edge_voice_name", "zh-CN-XiaoxiaoNeural-Female")

    # ç¡®ä¿ä¿å­˜çš„éŸ³è‰²åœ¨å¯ç”¨åˆ—è¡¨ä¸­
    if saved_voice_name not in friendly_names:
        # é€‰æ‹©ä¸UIè¯­è¨€åŒ¹é…çš„ç¬¬ä¸€ä¸ªè¯­éŸ³
        for v in edge_voices:
            if v.lower().startswith(st.session_state.get("ui_language", "zh-CN").lower()):
                saved_voice_name = v
                break
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
            saved_voice_name = edge_voices[0] if edge_voices else ""

    # éŸ³è‰²é€‰æ‹©ä¸‹æ‹‰æ¡†ï¼ˆEdge TTSéŸ³è‰²ç›¸å¯¹è¾ƒå°‘ï¼Œä¿ç•™ä¸‹æ‹‰æ¡†ï¼‰
    selected_friendly_name = st.selectbox(
        "éŸ³è‰²é€‰æ‹©",
        options=list(friendly_names.values()),
        index=list(friendly_names.keys()).index(saved_voice_name) if saved_voice_name in friendly_names else 0,
        help="é€‰æ‹©Edge TTSéŸ³è‰²"
    )

    # è·å–å®é™…çš„è¯­éŸ³åç§°
    voice_name = list(friendly_names.keys())[
        list(friendly_names.values()).index(selected_friendly_name)
    ]

    # æ˜¾ç¤ºéŸ³è‰²ä¿¡æ¯
    with st.expander("ğŸ’¡ Edge TTS éŸ³è‰²è¯´æ˜", expanded=False):
        st.write("**ä¸­æ–‡éŸ³è‰²ï¼š**")
        zh_voices = [v for v in edge_voices if v.startswith("zh-CN")]
        for v in zh_voices:
            gender = "å¥³å£°" if "Female" in v else "ç”·å£°"
            name = v.replace("-Female", "").replace("-Male", "").replace("zh-CN-", "").replace("Neural", "")
            st.write(f"â€¢ {name} ({gender})")

        st.write("")
        st.write("**è‹±æ–‡éŸ³è‰²ï¼š**")
        en_voices = [v for v in edge_voices if v.startswith("en-US")][:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
        for v in en_voices:
            gender = "å¥³å£°" if "Female" in v else "ç”·å£°"
            name = v.replace("-Female", "").replace("-Male", "").replace("en-US-", "").replace("Neural", "")
            st.write(f"â€¢ {name} ({gender})")

        if len([v for v in edge_voices if v.startswith("en-US")]) > 5:
            st.write("â€¢ ... æ›´å¤šè‹±æ–‡éŸ³è‰²")

    config.ui["edge_voice_name"] = voice_name
    config.ui["voice_name"] = voice_name  # å…¼å®¹æ€§

    # éŸ³é‡è°ƒèŠ‚
    voice_volume = st.slider(
        "éŸ³é‡è°ƒèŠ‚",
        min_value=0,
        max_value=100,
        value=int(config.ui.get("edge_volume", 80)),
        step=1,
        help="è°ƒèŠ‚è¯­éŸ³éŸ³é‡ (0-100)"
    )
    config.ui["edge_volume"] = voice_volume
    st.session_state['voice_volume'] = voice_volume / 100.0

    # è¯­é€Ÿè°ƒèŠ‚
    voice_rate = st.slider(
        "è¯­é€Ÿè°ƒèŠ‚",
        min_value=0.5,
        max_value=2.0,
        value=config.ui.get("edge_rate", 1.0),
        step=0.1,
        help="è°ƒèŠ‚è¯­éŸ³é€Ÿåº¦ (0.5-2.0å€é€Ÿ)"
    )
    config.ui["edge_rate"] = voice_rate
    st.session_state['voice_rate'] = voice_rate

    # è¯­è°ƒè°ƒèŠ‚
    voice_pitch = st.slider(
        "è¯­è°ƒè°ƒèŠ‚",
        min_value=-50,
        max_value=50,
        value=int(config.ui.get("edge_pitch", 0)),
        step=5,
        help="è°ƒèŠ‚è¯­éŸ³éŸ³è°ƒ (-50%åˆ°+50%)"
    )
    config.ui["edge_pitch"] = voice_pitch
    # è½¬æ¢ä¸ºæ¯”ä¾‹å€¼
    st.session_state['voice_pitch'] = 1.0 + (voice_pitch / 100.0)


def render_azure_speech_settings(tr):
    """æ¸²æŸ“ Azure Speech Services å¼•æ“è®¾ç½®"""
    # æœåŠ¡åŒºåŸŸé…ç½®
    azure_speech_region = st.text_input(
        "æœåŠ¡åŒºåŸŸ",
        value=config.azure.get("speech_region", ""),
        placeholder="ä¾‹å¦‚ï¼šeastus",
        help="Azure Speech Services æœåŠ¡åŒºåŸŸï¼Œå¦‚ï¼šeastus, westus2, eastasia ç­‰"
    )

    # API Keyé…ç½®
    azure_speech_key = st.text_input(
        "API Key",
        value=config.azure.get("speech_key", ""),
        type="password",
        help="Azure Speech Services API å¯†é’¥"
    )

    # ä¿å­˜Azureé…ç½®
    config.azure["speech_region"] = azure_speech_region
    config.azure["speech_key"] = azure_speech_key

    # éŸ³è‰²åç§°è¾“å…¥æ¡†
    saved_voice_name = config.ui.get("azure_voice_name", "zh-CN-XiaoxiaoMultilingualNeural")

    # éŸ³è‰²åç§°è¾“å…¥
    voice_name = st.text_input(
        "éŸ³è‰²åç§°",
        value=saved_voice_name,
        help="è¾“å…¥Azure Speech ServiceséŸ³è‰²åç§°ï¼Œç›´æ¥ä½¿ç”¨å®˜æ–¹éŸ³è‰²åç§°å³å¯ã€‚ä¾‹å¦‚ï¼šzh-CN-YunzeNeural",
        placeholder="zh-CN-YunzeNeural"
    )

    # æ˜¾ç¤ºå¸¸ç”¨éŸ³è‰²ç¤ºä¾‹
    with st.expander("ğŸ’¡ å¸¸ç”¨éŸ³è‰²å‚è€ƒ", expanded=False):
        st.write("**ä¸­æ–‡éŸ³è‰²ï¼š**")
        st.write("â€¢ zh-CN-XiaoxiaoMultilingualNeural (å¥³å£°ï¼Œå¤šè¯­è¨€)")
        st.write("â€¢ zh-CN-YunzeNeural (ç”·å£°)")
        st.write("â€¢ zh-CN-YunxiNeural (ç”·å£°)")
        st.write("â€¢ zh-CN-XiaochenNeural (å¥³å£°)")
        st.write("")
        st.write("**è‹±æ–‡éŸ³è‰²ï¼š**")
        st.write("â€¢ en-US-AndrewMultilingualNeural (ç”·å£°ï¼Œå¤šè¯­è¨€)")
        st.write("â€¢ en-US-AvaMultilingualNeural (å¥³å£°ï¼Œå¤šè¯­è¨€)")
        st.write("â€¢ en-US-BrianMultilingualNeural (ç”·å£°ï¼Œå¤šè¯­è¨€)")
        st.write("â€¢ en-US-EmmaMultilingualNeural (å¥³å£°ï¼Œå¤šè¯­è¨€)")
        st.write("")
        st.info("ğŸ’¡ æ›´å¤šéŸ³è‰²è¯·å‚è€ƒ [Azure Speech Services å®˜æ–¹æ–‡æ¡£](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support)")

    # å¿«é€Ÿé€‰æ‹©æŒ‰é’®
    st.write("**å¿«é€Ÿé€‰æ‹©ï¼š**")
    cols = st.columns(3)
    with cols[0]:
        if st.button("ä¸­æ–‡å¥³å£°", help="zh-CN-XiaoxiaoMultilingualNeural"):
            voice_name = "zh-CN-XiaoxiaoMultilingualNeural"
            st.rerun()
    with cols[1]:
        if st.button("ä¸­æ–‡ç”·å£°", help="zh-CN-YunzeNeural"):
            voice_name = "zh-CN-YunzeNeural"
            st.rerun()
    with cols[2]:
        if st.button("è‹±æ–‡å¥³å£°", help="en-US-AvaMultilingualNeural"):
            voice_name = "en-US-AvaMultilingualNeural"
            st.rerun()

    # éªŒè¯éŸ³è‰²åç§°å¹¶æ˜¾ç¤ºçŠ¶æ€
    if voice_name.strip():
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„AzureéŸ³è‰²æ ¼å¼
        if is_valid_azure_voice_name(voice_name):
            st.success(f"âœ… éŸ³è‰²åç§°æœ‰æ•ˆ: {voice_name}")
        else:
            st.warning(f"âš ï¸ éŸ³è‰²åç§°æ ¼å¼å¯èƒ½ä¸æ­£ç¡®: {voice_name}")
            st.info("ğŸ’¡ AzureéŸ³è‰²åç§°é€šå¸¸æ ¼å¼ä¸º: [è¯­è¨€]-[åœ°åŒº]-[åç§°]Neural")

    # ä¿å­˜é…ç½®
    config.ui["azure_voice_name"] = voice_name
    config.ui["voice_name"] = voice_name  # å…¼å®¹æ€§

    # éŸ³é‡è°ƒèŠ‚
    voice_volume = st.slider(
        "éŸ³é‡è°ƒèŠ‚",
        min_value=0,
        max_value=100,
        value=int(config.ui.get("azure_volume", 80)),
        step=1,
        help="è°ƒèŠ‚è¯­éŸ³éŸ³é‡ (0-100)"
    )
    config.ui["azure_volume"] = voice_volume
    st.session_state['voice_volume'] = voice_volume / 100.0

    # è¯­é€Ÿè°ƒèŠ‚
    voice_rate = st.slider(
        "è¯­é€Ÿè°ƒèŠ‚",
        min_value=0.5,
        max_value=2.0,
        value=config.ui.get("azure_rate", 1.0),
        step=0.1,
        help="è°ƒèŠ‚è¯­éŸ³é€Ÿåº¦ (0.5-2.0å€é€Ÿ)"
    )
    config.ui["azure_rate"] = voice_rate
    st.session_state['voice_rate'] = voice_rate

    # è¯­è°ƒè°ƒèŠ‚
    voice_pitch = st.slider(
        "è¯­è°ƒè°ƒèŠ‚",
        min_value=-50,
        max_value=50,
        value=int(config.ui.get("azure_pitch", 0)),
        step=5,
        help="è°ƒèŠ‚è¯­éŸ³éŸ³è°ƒ (-50%åˆ°+50%)"
    )
    config.ui["azure_pitch"] = voice_pitch
    # è½¬æ¢ä¸ºæ¯”ä¾‹å€¼
    st.session_state['voice_pitch'] = 1.0 + (voice_pitch / 100.0)

    # æ˜¾ç¤ºé…ç½®çŠ¶æ€
    if azure_speech_region and azure_speech_key:
        st.success("âœ… Azure Speech Services é…ç½®å·²è®¾ç½®")
    elif not azure_speech_region:
        st.warning("âš ï¸ è¯·é…ç½®æœåŠ¡åŒºåŸŸ")
    elif not azure_speech_key:
        st.warning("âš ï¸ è¯·é…ç½® API Key")


def render_tencent_tts_settings(tr):
    """æ¸²æŸ“è…¾è®¯äº‘ TTS å¼•æ“è®¾ç½®"""
    # Secret ID è¾“å…¥
    secret_id = st.text_input(
        "Secret ID",
        value=config.tencent.get("secret_id", ""),
        help="è¯·è¾“å…¥æ‚¨çš„è…¾è®¯äº‘ Secret ID"
    )

    # Secret Key è¾“å…¥
    secret_key = st.text_input(
        "Secret Key",
        value=config.tencent.get("secret_key", ""),
        type="password",
        help="è¯·è¾“å…¥æ‚¨çš„è…¾è®¯äº‘ Secret Key"
    )

    # åœ°åŸŸé€‰æ‹©
    region_options = [
        "ap-beijing",
        "ap-shanghai",
        "ap-guangzhou",
        "ap-chengdu",
        "ap-nanjing",
        "ap-singapore",
        "ap-hongkong"
    ]
    
    saved_region = config.tencent.get("region", "ap-beijing")
    if saved_region not in region_options:
        region_options.append(saved_region)
    
    region = st.selectbox(
        "æœåŠ¡åœ°åŸŸ",
        options=region_options,
        index=region_options.index(saved_region),
        help="é€‰æ‹©è…¾è®¯äº‘ TTS æœåŠ¡åœ°åŸŸ"
    )

    # éŸ³è‰²é€‰æ‹©
    voice_type_options = {
        "101001": "æ™ºç‘œ - å¥³å£°ï¼ˆæ¨èï¼‰",
        "101002": "æ™ºè† - å¥³å£°",
        "101003": "æ™ºç¾ - å¥³å£°",
        "101004": "æ™ºäº‘ - ç”·å£°",
        "101005": "æ™ºè‰ - å¥³å£°",
        "101006": "æ™ºè¨€ - ç”·å£°",
        "101007": "æ™ºå¨œ - å¥³å£°",
        "101008": "æ™ºçª - å¥³å£°",
        "101009": "æ™ºèŠ¸ - å¥³å£°",
        "101010": "æ™ºå - ç”·å£°",
        "101011": "æ™ºç‡• - å¥³å£°",
        "101012": "æ™ºä¸¹ - å¥³å£°",
        "101013": "æ™ºè¾‰ - ç”·å£°",
        "101014": "æ™ºå® - å¥³å£°",
        "101015": "æ™ºèŒ - å¥³å£°",
        "101016": "æ™ºç”œ - å¥³å£°",
        "101017": "æ™ºè“‰ - å¥³å£°",
        "101018": "æ™ºé– - ç”·å£°"
    }
    
    saved_voice_type = config.ui.get("tencent_voice_type", "101001")
    if saved_voice_type not in voice_type_options:
        voice_type_options[saved_voice_type] = f"è‡ªå®šä¹‰éŸ³è‰² ({saved_voice_type})"
    
    selected_voice_display = st.selectbox(
        "éŸ³è‰²é€‰æ‹©",
        options=list(voice_type_options.values()),
        index=list(voice_type_options.keys()).index(saved_voice_type),
        help="é€‰æ‹©è…¾è®¯äº‘ TTS éŸ³è‰²"
    )
    
    # è·å–å®é™…çš„éŸ³è‰²ID
    voice_type = list(voice_type_options.keys())[
        list(voice_type_options.values()).index(selected_voice_display)
    ]
    
    # è¯­é€Ÿè°ƒèŠ‚
    voice_rate = st.slider(
        "è¯­é€Ÿè°ƒèŠ‚",
        min_value=0.5,
        max_value=2.0,
        value=config.ui.get("tencent_rate", 1.0),
        step=0.1,
        help="è°ƒèŠ‚è¯­éŸ³é€Ÿåº¦ (0.5-2.0)"
    )
    
    config.ui["voice_name"] = saved_voice_type  # å…¼å®¹æ€§
    
    # æ˜¾ç¤ºéŸ³è‰²è¯´æ˜
    with st.expander("ğŸ’¡ è…¾è®¯äº‘ TTS éŸ³è‰²è¯´æ˜", expanded=False):
        st.write("**å¥³å£°éŸ³è‰²ï¼š**")
        female_voices = [(k, v) for k, v in voice_type_options.items() if "å¥³å£°" in v]
        for voice_id, voice_desc in female_voices[:6]:  # æ˜¾ç¤ºå‰6ä¸ª
            st.write(f"â€¢ {voice_desc} (ID: {voice_id})")
        
        st.write("")
        st.write("**ç”·å£°éŸ³è‰²ï¼š**")
        male_voices = [(k, v) for k, v in voice_type_options.items() if "ç”·å£°" in v]
        for voice_id, voice_desc in male_voices:
            st.write(f"â€¢ {voice_desc} (ID: {voice_id})")
        
        st.write("")
        st.info("ğŸ’¡ æ›´å¤šéŸ³è‰²è¯·å‚è€ƒè…¾è®¯äº‘å®˜æ–¹æ–‡æ¡£")
    
    # ä¿å­˜é…ç½®
    config.tencent["secret_id"] = secret_id
    config.tencent["secret_key"] = secret_key
    config.tencent["region"] = region
    config.ui["tencent_voice_type"] = voice_type
    config.ui["tencent_rate"] = voice_rate
    config.ui["voice_name"] = saved_voice_type #å…¼å®¹æ€§


def render_qwen3_tts_settings(tr):
    """æ¸²æŸ“ Qwen3 TTS è®¾ç½®"""
    api_key = st.text_input(
        "API Key",
        value=config.tts_qwen.get("api_key", ""),
        type="password",
        help="é€šä¹‰åƒé—® DashScope API Key"
    )

    model_name = st.text_input(
        "æ¨¡å‹åç§°",
        value=config.tts_qwen.get("model_name", "qwen3-tts-flash"),
        help="Qwen TTS æ¨¡å‹åï¼Œä¾‹å¦‚ qwen3-tts-flash"
    )

    # Qwen3 TTS éŸ³è‰²é€‰é¡¹ - ä¸­æ–‡å: è‹±æ–‡å‚æ•°
    voice_options = {
        "èŠŠæ‚¦": "Cherry",
        "æ™¨ç…¦": "Ethan",
        "ä¸åƒé±¼": "Nofish",
        "è©¹å¦®å¼—": "Jennifer",
        "ç”œèŒ¶": "Ryan",
        "å¡æ·ç³å¨œ": "Katerina",
        "å¢¨è®²å¸ˆ": "Elias",
        "ä¸Šæµ·-é˜¿ç": "Jada",
        "åŒ—äº¬-æ™“ä¸œ": "Dylan",
        "å››å·-æ™´å„¿": "Sunny",
        "å—äº¬-è€æ": "Li",
        "é™•è¥¿-ç§¦å·": "Marcus",
        "é—½å—-é˜¿æ°": "Roy",
        "å¤©æ´¥-æå½¼å¾—": "Peter",
        "ç²¤è¯­-é˜¿å¼º": "Rocky",
        "ç²¤è¯­-é˜¿æ¸…": "Kiki",
        "å››å·-ç¨‹å·": "Eric"
    }
    
    # æ˜¾ç¤ºç»™ç”¨æˆ·çš„ä¸­æ–‡åç§°åˆ—è¡¨
    display_names = list(voice_options.keys())
    saved_voice_param = config.ui.get("qwen_voice_type", "Cherry")
    
    # å¦‚æœä¿å­˜çš„è‹±æ–‡å‚æ•°ä¸åœ¨é€‰é¡¹ä¸­ï¼ŒæŸ¥æ‰¾å¯¹åº”çš„ä¸­æ–‡åç§°
    saved_display_name = "èŠŠæ‚¦"  # é»˜è®¤å€¼
    for chinese_name, english_param in voice_options.items():
        if english_param == saved_voice_param:
            saved_display_name = chinese_name
            break
    
    # å¦‚æœä¿å­˜çš„éŸ³è‰²ä¸åœ¨é€‰é¡¹ä¸­ï¼Œæ·»åŠ åˆ°è‡ªå®šä¹‰é€‰é¡¹
    if saved_display_name not in display_names:
        display_names.append(saved_display_name)
        voice_options[saved_display_name] = saved_voice_param

    selected_display_name = st.selectbox(
        "éŸ³è‰²é€‰æ‹©",
        options=display_names,
        index=display_names.index(saved_display_name) if saved_display_name in display_names else 0,
        help="é€‰æ‹©Qwen3 TTSéŸ³è‰²"
    )
    
    # è·å–å¯¹åº”çš„è‹±æ–‡å‚æ•°
    voice_type = voice_options.get(selected_display_name, "Cherry")

    voice_rate = st.slider(
        "è¯­é€Ÿè°ƒèŠ‚",
        min_value=0.5,
        max_value=2.0,
        value=1.0,
        step=0.1,
        help="è°ƒèŠ‚è¯­éŸ³é€Ÿåº¦ (0.5-2.0)"
    )

    # ä¿å­˜é…ç½®
    config.tts_qwen["api_key"] = api_key
    config.tts_qwen["model_name"] = model_name
    config.ui["qwen_voice_type"] = voice_type
    config.ui["qwen3_rate"] = voice_rate
    config.ui["voice_name"] = voice_type #å…¼å®¹æ€§


def render_indextts2_tts_settings(tr):
    """æ¸²æŸ“ IndexTTS2 TTS è®¾ç½®"""
    import os
    
    # API åœ°å€é…ç½®
    api_url = st.text_input(
        "API åœ°å€",
        value=config.indextts2.get("api_url", "http://127.0.0.1:8081/tts"),
        help="IndexTTS2 API æœåŠ¡åœ°å€"
    )
    
    # å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    reference_audio = st.text_input(
        "å‚è€ƒéŸ³é¢‘è·¯å¾„",
        value=config.indextts2.get("reference_audio", ""),
        help="ç”¨äºè¯­éŸ³å…‹éš†çš„å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆWAV æ ¼å¼ï¼Œå»ºè®® 3-10 ç§’ï¼‰"
    )
    
    # æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
    uploaded_file = st.file_uploader(
        "æˆ–ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶",
        type=["wav", "mp3"],
        help="ä¸Šä¼ ä¸€æ®µæ¸…æ™°çš„éŸ³é¢‘ç”¨äºè¯­éŸ³å…‹éš†"
    )
    
    if uploaded_file is not None:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        import tempfile
        temp_dir = tempfile.gettempdir()
        audio_path = os.path.join(temp_dir, f"indextts2_ref_{uploaded_file.name}")
        with open(audio_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        reference_audio = audio_path
        st.success(f"âœ… éŸ³é¢‘å·²ä¸Šä¼ : {audio_path}")
    
    # æ¨ç†æ¨¡å¼
    infer_mode = st.selectbox(
        "æ¨ç†æ¨¡å¼",
        options=["æ™®é€šæ¨ç†", "å¿«é€Ÿæ¨ç†"],
        index=0 if config.indextts2.get("infer_mode", "æ™®é€šæ¨ç†") == "æ™®é€šæ¨ç†" else 1,
        help="æ™®é€šæ¨ç†è´¨é‡æ›´é«˜ä½†é€Ÿåº¦è¾ƒæ…¢ï¼Œå¿«é€Ÿæ¨ç†é€Ÿåº¦æ›´å¿«ä½†è´¨é‡ç•¥ä½"
    )
    
    # é«˜çº§å‚æ•°æŠ˜å é¢æ¿
    with st.expander("ğŸ”§ é«˜çº§å‚æ•°", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            temperature = st.slider(
                "é‡‡æ ·æ¸©åº¦ (Temperature)",
                min_value=0.1,
                max_value=2.0,
                value=float(config.indextts2.get("temperature", 1.0)),
                step=0.1,
                help="æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¾“å‡ºè¶Šéšæœºï¼Œå€¼è¶Šä½è¶Šç¡®å®š"
            )
            
            top_p = st.slider(
                "Top P",
                min_value=0.0,
                max_value=1.0,
                value=float(config.indextts2.get("top_p", 0.8)),
                step=0.05,
                help="nucleus é‡‡æ ·çš„æ¦‚ç‡é˜ˆå€¼ï¼Œå€¼è¶Šå°ç»“æœè¶Šç¡®å®š"
            )
            
            top_k = st.slider(
                "Top K",
                min_value=0,
                max_value=100,
                value=int(config.indextts2.get("top_k", 30)),
                step=5,
                help="top-k é‡‡æ ·çš„ k å€¼ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨ top-k"
            )
        
        with col2:
            num_beams = st.slider(
                "æŸæœç´¢ (Num Beams)",
                min_value=1,
                max_value=10,
                value=int(config.indextts2.get("num_beams", 3)),
                step=1,
                help="æŸæœç´¢çš„ beam æ•°é‡ï¼Œå€¼è¶Šå¤§è´¨é‡å¯èƒ½è¶Šå¥½ä½†é€Ÿåº¦è¶Šæ…¢"
            )
            
            repetition_penalty = st.slider(
                "é‡å¤æƒ©ç½š (Repetition Penalty)",
                min_value=1.0,
                max_value=20.0,
                value=float(config.indextts2.get("repetition_penalty", 10.0)),
                step=0.5,
                help="å€¼è¶Šå¤§è¶Šèƒ½é¿å…é‡å¤ï¼Œä½†è¿‡å¤§å¯èƒ½å¯¼è‡´ä¸è‡ªç„¶"
            )
            
            do_sample = st.checkbox(
                "å¯ç”¨é‡‡æ ·",
                value=config.indextts2.get("do_sample", True),
                help="å¯ç”¨é‡‡æ ·å¯ä»¥è·å¾—æ›´è‡ªç„¶çš„è¯­éŸ³"
            )
    
    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ’¡ IndexTTS2 ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        **é›¶æ ·æœ¬è¯­éŸ³å…‹éš†**
        
        1. **å‡†å¤‡å‚è€ƒéŸ³é¢‘**ï¼šä¸Šä¼ æˆ–æŒ‡å®šä¸€æ®µæ¸…æ™°çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆå»ºè®® 3-10 ç§’ï¼‰
        2. **è®¾ç½® API åœ°å€**ï¼šç¡®ä¿ IndexTTS2 æœåŠ¡æ­£å¸¸è¿è¡Œ
        3. **å¼€å§‹åˆæˆ**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨å‚è€ƒéŸ³é¢‘çš„éŸ³è‰²åˆæˆæ–°è¯­éŸ³
        
        **æ³¨æ„äº‹é¡¹**ï¼š
        - å‚è€ƒéŸ³é¢‘è´¨é‡ç›´æ¥å½±å“åˆæˆæ•ˆæœ
        - å»ºè®®ä½¿ç”¨æ— èƒŒæ™¯å™ªéŸ³çš„æ¸…æ™°éŸ³é¢‘
        - æ–‡æœ¬é•¿åº¦å»ºè®®æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
        - é¦–æ¬¡åˆæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        """)
    
    # ä¿å­˜é…ç½®
    config.indextts2["api_url"] = api_url
    config.indextts2["reference_audio"] = reference_audio
    config.indextts2["infer_mode"] = infer_mode
    config.indextts2["temperature"] = temperature
    config.indextts2["top_p"] = top_p
    config.indextts2["top_k"] = top_k
    config.indextts2["num_beams"] = num_beams
    config.indextts2["repetition_penalty"] = repetition_penalty
    config.indextts2["do_sample"] = do_sample
    
    # ä¿å­˜ voice_name ç”¨äºå…¼å®¹æ€§
    if reference_audio:
        config.ui["voice_name"] = f"indextts2:{reference_audio}"


def render_voice_preview_new(tr, selected_engine):
    """æ¸²æŸ“æ–°çš„è¯­éŸ³è¯•å¬åŠŸèƒ½"""
    if st.button("ğŸµ è¯•å¬è¯­éŸ³åˆæˆ", use_container_width=True):
        play_content = "æ„Ÿè°¢å…³æ³¨ NarratoAIï¼Œæœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œå¯ä»¥å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼Œæ±‚åŠ©æˆ–è®¨è®º"

        # æ ¹æ®é€‰æ‹©çš„å¼•æ“è·å–å¯¹åº”çš„è¯­éŸ³é…ç½®
        voice_name = ""
        voice_rate = 1.0
        voice_pitch = 1.0

        if selected_engine == "edge_tts":
            voice_name = config.ui.get("edge_voice_name", "zh-CN-XiaoyiNeural-Female")
            voice_rate = config.ui.get("edge_rate", 1.0)
            voice_pitch = 1.0 + (config.ui.get("edge_pitch", 0) / 100.0)
        elif selected_engine == "azure_speech":
            voice_name = config.ui.get("azure_voice_name", "zh-CN-XiaoxiaoMultilingualNeural")
            voice_rate = config.ui.get("azure_rate", 1.0)
            voice_pitch = 1.0 + (config.ui.get("azure_pitch", 0) / 100.0)
        elif selected_engine == "soulvoice":
            voice_uri = config.soulvoice.get("voice_uri", "")
            if voice_uri:
                if not voice_uri.startswith("soulvoice:") and not voice_uri.startswith("speech:"):
                    voice_name = f"soulvoice:{voice_uri}"
                else:
                    voice_name = voice_uri if voice_uri.startswith("soulvoice:") else f"soulvoice:{voice_uri}"
            voice_rate = 1.0  # SoulVoice ä½¿ç”¨é»˜è®¤è¯­é€Ÿ
            voice_pitch = 1.0  # SoulVoice ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚
        elif selected_engine == "tencent_tts":
            voice_type = config.ui.get("tencent_voice_type", "101001")
            voice_name = f"tencent:{voice_type}"
            voice_rate = config.ui.get("tencent_rate", 1.0)
            voice_pitch = 1.0  # è…¾è®¯äº‘ TTS ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚
        elif selected_engine == "qwen3_tts":
            vt = config.ui.get("qwen_voice_type", "Cherry")
            voice_name = f"qwen3:{vt}"
            voice_rate = config.ui.get("qwen3_rate", 1.0)
            voice_pitch = 1.0  # Qwen3 TTS ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚
        elif selected_engine == "indextts2":
            reference_audio = config.indextts2.get("reference_audio", "")
            if reference_audio:
                voice_name = f"indextts2:{reference_audio}"
            voice_rate = 1.0  # IndexTTS2 ä¸æ”¯æŒé€Ÿåº¦è°ƒèŠ‚
            voice_pitch = 1.0  # IndexTTS2 ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚

        if not voice_name:
            st.error("è¯·å…ˆé…ç½®è¯­éŸ³è®¾ç½®")
            return

        with st.spinner("æ­£åœ¨åˆæˆè¯­éŸ³..."):
            temp_dir = utils.storage_dir("temp", create=True)
            audio_file = os.path.join(temp_dir, f"tmp-voice-{str(uuid4())}.mp3")

            sub_maker = voice.tts(
                text=play_content,
                voice_name=voice_name,
                voice_rate=voice_rate,
                voice_pitch=voice_pitch,
                voice_file=audio_file,
                tts_engine=st.session_state.get('tts_engine')
            )

            if sub_maker and os.path.exists(audio_file):
                st.success("âœ… è¯­éŸ³åˆæˆæˆåŠŸï¼")

                # æ’­æ”¾éŸ³é¢‘
                with open(audio_file, 'rb') as audio_file_obj:
                    audio_bytes = audio_file_obj.read()
                    st.audio(audio_bytes, format='audio/mp3')

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(audio_file)
                except:
                    pass
            else:
                st.error("âŒ è¯­éŸ³åˆæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")


def render_azure_v2_settings(tr):
    """æ¸²æŸ“Azure V2è¯­éŸ³è®¾ç½®ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    saved_azure_speech_region = config.azure.get("speech_region", "")
    saved_azure_speech_key = config.azure.get("speech_key", "")

    azure_speech_region = st.text_input(
        tr("Speech Region"),
        value=saved_azure_speech_region
    )
    azure_speech_key = st.text_input(
        tr("Speech Key"),
        value=saved_azure_speech_key,
        type="password"
    )

    config.azure["speech_region"] = azure_speech_region
    config.azure["speech_key"] = azure_speech_key


def render_voice_parameters(tr, voice_name):
    """æ¸²æŸ“è¯­éŸ³å‚æ•°è®¾ç½®ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    # éŸ³é‡ - ä½¿ç”¨ç»Ÿä¸€çš„é»˜è®¤å€¼
    voice_volume = st.slider(
        tr("Speech Volume"),
        min_value=AudioVolumeDefaults.MIN_VOLUME,
        max_value=AudioVolumeDefaults.MAX_VOLUME,
        value=AudioVolumeDefaults.VOICE_VOLUME,
        step=0.01,
        help=tr("Adjust the volume of the original audio")
    )
    st.session_state['voice_volume'] = voice_volume

    # æ£€æŸ¥æ˜¯å¦ä¸º SoulVoice å¼•æ“
    is_soulvoice = voice.is_soulvoice_voice(voice_name)

    # è¯­é€Ÿ
    if is_soulvoice:
        # SoulVoice æ”¯æŒæ›´ç²¾ç»†çš„è¯­é€Ÿæ§åˆ¶
        voice_rate = st.slider(
            tr("Speech Rate"),
            min_value=0.5,
            max_value=2.0,
            value=1.0,
            step=0.1,
            help="SoulVoice è¯­éŸ³é€Ÿåº¦æ§åˆ¶"
        )
    else:
        # Azure TTS ä½¿ç”¨é¢„è®¾é€‰é¡¹
        voice_rate = st.selectbox(
            tr("Speech Rate"),
            options=[0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5, 1.8, 2.0],
            index=2,
        )
    st.session_state['voice_rate'] = voice_rate

    # éŸ³è°ƒ - SoulVoice ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚
    if not is_soulvoice:
        voice_pitch = st.selectbox(
            tr("Speech Pitch"),
            options=[0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5, 1.8, 2.0],
            index=2,
        )
        st.session_state['voice_pitch'] = voice_pitch
    else:
        # SoulVoice ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚ï¼Œè®¾ç½®é»˜è®¤å€¼
        st.session_state['voice_pitch'] = 1.0
        st.info("â„¹ï¸ SoulVoice å¼•æ“ä¸æ”¯æŒéŸ³è°ƒè°ƒèŠ‚")


def render_voice_preview(tr, voice_name):
    """æ¸²æŸ“è¯­éŸ³è¯•å¬åŠŸèƒ½"""
    if st.button(tr("Play Voice")):
        play_content = "æ„Ÿè°¢å…³æ³¨ NarratoAIï¼Œæœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œå¯ä»¥å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼Œæ±‚åŠ©æˆ–è®¨è®º"
        if not play_content:
            play_content = st.session_state.get('video_script', '')
        if not play_content:
            play_content = tr("Voice Example")

        with st.spinner(tr("Synthesizing Voice")):
            temp_dir = utils.storage_dir("temp", create=True)
            audio_file = os.path.join(temp_dir, f"tmp-voice-{str(uuid4())}.mp3")

            sub_maker = voice.tts(
                text=play_content,
                voice_name=voice_name,
                voice_rate=st.session_state.get('voice_rate', 1.0),
                voice_pitch=st.session_state.get('voice_pitch', 1.0),
                voice_file=audio_file,
            )

            # å¦‚æœè¯­éŸ³æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å†…å®¹é‡è¯•
            if not sub_maker:
                play_content = "This is a example voice. if you hear this, the voice synthesis failed with the original content."
                sub_maker = voice.tts(
                    text=play_content,
                    voice_name=voice_name,
                    voice_rate=st.session_state.get('voice_rate', 1.0),
                    voice_pitch=st.session_state.get('voice_pitch', 1.0),
                    voice_file=audio_file,
                )

            if sub_maker and os.path.exists(audio_file):
                st.success(tr("Voice synthesis successful"))
                st.audio(audio_file, format="audio/mp3")
                if os.path.exists(audio_file):
                    os.remove(audio_file)
            else:
                st.error(tr("Voice synthesis failed"))


def render_bgm_settings(tr):
    """æ¸²æŸ“èƒŒæ™¯éŸ³ä¹è®¾ç½®"""
    # èƒŒæ™¯éŸ³ä¹é€‰é¡¹
    bgm_options = [
        (tr("No Background Music"), ""),
        (tr("Random Background Music"), "random"),
        (tr("Custom Background Music"), "custom"),
    ]

    selected_index = st.selectbox(
        tr("Background Music"),
        index=1,
        options=range(len(bgm_options)),
        format_func=lambda x: bgm_options[x][0],
    )

    # è·å–é€‰æ‹©çš„èƒŒæ™¯éŸ³ä¹ç±»å‹
    bgm_type = bgm_options[selected_index][1]
    st.session_state['bgm_type'] = bgm_type

    # è‡ªå®šä¹‰èƒŒæ™¯éŸ³ä¹å¤„ç†
    if bgm_type == "custom":
        custom_bgm_file = st.text_input(tr("Custom Background Music File"))
        if custom_bgm_file and os.path.exists(custom_bgm_file):
            st.session_state['bgm_file'] = custom_bgm_file

    # èƒŒæ™¯éŸ³ä¹éŸ³é‡ - ä½¿ç”¨ç»Ÿä¸€çš„é»˜è®¤å€¼
    bgm_volume = st.slider(
        tr("Background Music Volume"),
        min_value=AudioVolumeDefaults.MIN_VOLUME,
        max_value=AudioVolumeDefaults.MAX_VOLUME,
        value=AudioVolumeDefaults.BGM_VOLUME,
        step=0.01,
        help=tr("Adjust the volume of the original audio")
    )
    st.session_state['bgm_volume'] = bgm_volume


def get_audio_params():
    """è·å–éŸ³é¢‘å‚æ•°"""
    return {
        'voice_name': config.ui.get("voice_name", ""),
        'voice_volume': st.session_state.get('voice_volume', AudioVolumeDefaults.VOICE_VOLUME),
        'voice_rate': st.session_state.get('voice_rate', 1.0),
        'voice_pitch': st.session_state.get('voice_pitch', 1.0),
        'bgm_type': st.session_state.get('bgm_type', 'random'),
        'bgm_file': st.session_state.get('bgm_file', ''),
        'bgm_volume': st.session_state.get('bgm_volume', AudioVolumeDefaults.BGM_VOLUME),
        'tts_engine': st.session_state.get('tts_engine', "edge_tts"),
    }
