# NarratoAI å¤§æ¨¡å‹æœåŠ¡ä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

NarratoAI é¡¹ç›®å·²å®Œæˆå¤§æ¨¡å‹æœåŠ¡çš„å…¨é¢é‡æ„ï¼Œæä¾›äº†ç»Ÿä¸€ã€æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„å¤§æ¨¡å‹é›†æˆæ¶æ„ã€‚æ–°æ¶æ„æ”¯æŒå¤šç§å¤§æ¨¡å‹ä¾›åº”å•†ï¼Œå…·æœ‰ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼éªŒè¯å’Œå®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ã€‚

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

### æ ¸å¿ƒç»„ä»¶

```
app/services/llm/
â”œâ”€â”€ __init__.py              # æ¨¡å—å…¥å£
â”œâ”€â”€ base.py                  # æŠ½è±¡åŸºç±»
â”œâ”€â”€ manager.py               # æœåŠ¡ç®¡ç†å™¨
â”œâ”€â”€ unified_service.py       # ç»Ÿä¸€æœåŠ¡æ¥å£
â”œâ”€â”€ validators.py            # è¾“å‡ºæ ¼å¼éªŒè¯å™¨
â”œâ”€â”€ exceptions.py            # å¼‚å¸¸ç±»å®šä¹‰
â”œâ”€â”€ migration_adapter.py     # è¿ç§»é€‚é…å™¨
â”œâ”€â”€ config_validator.py      # é…ç½®éªŒè¯å™¨
â”œâ”€â”€ test_llm_service.py      # æµ‹è¯•è„šæœ¬
â””â”€â”€ providers/               # æä¾›å•†å®ç°
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ gemini_provider.py
    â”œâ”€â”€ gemini_openai_provider.py
    â”œâ”€â”€ openai_provider.py
    â”œâ”€â”€ qwen_provider.py
    â”œâ”€â”€ deepseek_provider.py
    â””â”€â”€ siliconflow_provider.py
```

### æ”¯æŒçš„ä¾›åº”å•†

#### è§†è§‰æ¨¡å‹ä¾›åº”å•†
- **Gemini** (åŸç”ŸAPI + OpenAIå…¼å®¹)
- **QwenVL** (é€šä¹‰åƒé—®è§†è§‰)
- **Siliconflow** (ç¡…åŸºæµåŠ¨)

#### æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¾›åº”å•†
- **OpenAI** (æ ‡å‡†OpenAI API)
- **Gemini** (åŸç”ŸAPI + OpenAIå…¼å®¹)
- **DeepSeek** (æ·±åº¦æ±‚ç´¢)
- **Qwen** (é€šä¹‰åƒé—®)
- **Siliconflow** (ç¡…åŸºæµåŠ¨)

## âš™ï¸ é…ç½®è¯´æ˜

### é…ç½®æ–‡ä»¶æ ¼å¼

åœ¨ `config.toml` ä¸­é…ç½®å¤§æ¨¡å‹æœåŠ¡ï¼š

```toml
[app]
    # è§†è§‰æ¨¡å‹æä¾›å•†é…ç½®
    vision_llm_provider = "gemini"
    
    # Gemini è§†è§‰æ¨¡å‹
    vision_gemini_api_key = "your_gemini_api_key"
    vision_gemini_model_name = "gemini-2.0-flash-lite"
    vision_gemini_base_url = "https://generativelanguage.googleapis.com/v1beta"
    
    # QwenVL è§†è§‰æ¨¡å‹
    vision_qwenvl_api_key = "your_qwen_api_key"
    vision_qwenvl_model_name = "qwen2.5-vl-32b-instruct"
    vision_qwenvl_base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    
    # æ–‡æœ¬æ¨¡å‹æä¾›å•†é…ç½®
    text_llm_provider = "openai"
    
    # OpenAI æ–‡æœ¬æ¨¡å‹
    text_openai_api_key = "your_openai_api_key"
    text_openai_model_name = "gpt-4o-mini"
    text_openai_base_url = "https://api.openai.com/v1"
    
    # DeepSeek æ–‡æœ¬æ¨¡å‹
    text_deepseek_api_key = "your_deepseek_api_key"
    text_deepseek_model_name = "deepseek-chat"
    text_deepseek_base_url = "https://api.deepseek.com"
```

### é…ç½®éªŒè¯

ä½¿ç”¨é…ç½®éªŒè¯å™¨æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®ï¼š

```python
from app.services.llm.config_validator import LLMConfigValidator

# éªŒè¯æ‰€æœ‰é…ç½®
results = LLMConfigValidator.validate_all_configs()

# æ‰“å°éªŒè¯æŠ¥å‘Š
LLMConfigValidator.print_validation_report(results)

# è·å–é…ç½®å»ºè®®
suggestions = LLMConfigValidator.get_config_suggestions()
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. ç»Ÿä¸€æœåŠ¡æ¥å£ï¼ˆæ¨èï¼‰

```python
from app.services.llm.unified_service import UnifiedLLMService

# å›¾ç‰‡åˆ†æ
results = await UnifiedLLMService.analyze_images(
    images=["path/to/image1.jpg", "path/to/image2.jpg"],
    prompt="è¯·æè¿°è¿™äº›å›¾ç‰‡çš„å†…å®¹",
    provider="gemini",  # å¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    batch_size=10
)

# æ–‡æœ¬ç”Ÿæˆ
text = await UnifiedLLMService.generate_text(
    prompt="è¯·ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIä¸“å®¶",
    provider="openai",  # å¯é€‰
    temperature=0.7,
    response_format="json"  # å¯é€‰ï¼Œæ”¯æŒJSONæ ¼å¼è¾“å‡º
)

# è§£è¯´æ–‡æ¡ˆç”Ÿæˆï¼ˆå¸¦éªŒè¯ï¼‰
narration_items = await UnifiedLLMService.generate_narration_script(
    prompt="æ ¹æ®è§†é¢‘å†…å®¹ç”Ÿæˆè§£è¯´æ–‡æ¡ˆ...",
    validate_output=True  # è‡ªåŠ¨éªŒè¯è¾“å‡ºæ ¼å¼
)

# å­—å¹•åˆ†æ
analysis = await UnifiedLLMService.analyze_subtitle(
    subtitle_content="å­—å¹•å†…å®¹...",
    validate_output=True
)
```

### 2. ç›´æ¥ä½¿ç”¨æœåŠ¡ç®¡ç†å™¨

```python
from app.services.llm.manager import LLMServiceManager

# è·å–è§†è§‰æ¨¡å‹æä¾›å•†
vision_provider = LLMServiceManager.get_vision_provider("gemini")
results = await vision_provider.analyze_images(images, prompt)

# è·å–æ–‡æœ¬æ¨¡å‹æä¾›å•†
text_provider = LLMServiceManager.get_text_provider("openai")
text = await text_provider.generate_text(prompt)
```

### 3. è¿ç§»é€‚é…å™¨ï¼ˆå‘åå…¼å®¹ï¼‰

```python
from app.services.llm.migration_adapter import create_vision_analyzer

# å…¼å®¹æ—§çš„æ¥å£
analyzer = create_vision_analyzer("gemini", api_key, model, base_url)
results = await analyzer.analyze_images(images, prompt)
```

## ğŸ” è¾“å‡ºæ ¼å¼éªŒè¯

### è§£è¯´æ–‡æ¡ˆéªŒè¯

```python
from app.services.llm.validators import OutputValidator

# éªŒè¯è§£è¯´æ–‡æ¡ˆæ ¼å¼
try:
    narration_items = OutputValidator.validate_narration_script(output)
    print(f"éªŒè¯æˆåŠŸï¼Œå…± {len(narration_items)} ä¸ªç‰‡æ®µ")
except ValidationError as e:
    print(f"éªŒè¯å¤±è´¥: {e.message}")
```

### JSONè¾“å‡ºéªŒè¯

```python
# éªŒè¯JSONæ ¼å¼
try:
    data = OutputValidator.validate_json_output(output)
    print("JSONæ ¼å¼éªŒè¯æˆåŠŸ")
except ValidationError as e:
    print(f"JSONéªŒè¯å¤±è´¥: {e.message}")
```

## ğŸ§ª æµ‹è¯•å’Œè°ƒè¯•

### è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
# è¿è¡Œå®Œæ•´çš„LLMæœåŠ¡æµ‹è¯•
python app/services/llm/test_llm_service.py
```

æµ‹è¯•è„šæœ¬ä¼šéªŒè¯ï¼š
- é…ç½®æœ‰æ•ˆæ€§
- æä¾›å•†ä¿¡æ¯è·å–
- æ–‡æœ¬ç”ŸæˆåŠŸèƒ½
- JSONæ ¼å¼ç”Ÿæˆ
- å­—å¹•åˆ†æåŠŸèƒ½
- è§£è¯´æ–‡æ¡ˆç”ŸæˆåŠŸèƒ½

### è°ƒè¯•æŠ€å·§

1. **å¯ç”¨è¯¦ç»†æ—¥å¿—**ï¼š
```python
from loguru import logger
logger.add("llm_service.log", level="DEBUG")
```

2. **æ¸…ç©ºæä¾›å•†ç¼“å­˜**ï¼š
```python
UnifiedLLMService.clear_cache()
```

3. **æ£€æŸ¥æä¾›å•†ä¿¡æ¯**ï¼š
```python
info = UnifiedLLMService.get_provider_info()
print(info)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. APIå¯†é’¥å®‰å…¨
- ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç APIå¯†é’¥
- ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ç®¡ç†å¯†é’¥
- å®šæœŸè½®æ¢APIå¯†é’¥

### 2. é”™è¯¯å¤„ç†
- æ‰€æœ‰LLMæœåŠ¡è°ƒç”¨éƒ½åº”è¯¥åŒ…è£…åœ¨try-catchä¸­
- ä½¿ç”¨é€‚å½“çš„å¼‚å¸¸ç±»å‹è¿›è¡Œé”™è¯¯å¤„ç†
- å®ç°é‡è¯•æœºåˆ¶å¤„ç†ä¸´æ—¶æ€§é”™è¯¯

### 3. æ€§èƒ½ä¼˜åŒ–
- åˆç†è®¾ç½®æ‰¹å¤„ç†å¤§å°
- ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è°ƒç”¨
- ç›‘æ§APIè°ƒç”¨é¢‘ç‡å’Œæˆæœ¬

### 4. æ¨¡å‹é€‰æ‹©
- æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡å‹
- è€ƒè™‘æˆæœ¬å’Œæ€§èƒ½çš„å¹³è¡¡
- å®šæœŸæ›´æ–°åˆ°æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬

## ğŸ”§ æ‰©å±•æ–°ä¾›åº”å•†

### 1. åˆ›å»ºæä¾›å•†ç±»

```python
# app/services/llm/providers/new_provider.py
from ..base import TextModelProvider

class NewTextProvider(TextModelProvider):
    @property
    def provider_name(self) -> str:
        return "new_provider"
    
    @property
    def supported_models(self) -> List[str]:
        return ["model-1", "model-2"]
    
    async def generate_text(self, prompt: str, **kwargs) -> str:
        # å®ç°å…·ä½“çš„APIè°ƒç”¨é€»è¾‘
        pass
```

### 2. æ³¨å†Œæä¾›å•†

```python
# app/services/llm/providers/__init__.py
from .new_provider import NewTextProvider

LLMServiceManager.register_text_provider('new_provider', NewTextProvider)
```

### 3. æ·»åŠ é…ç½®æ”¯æŒ

```toml
# config.toml
text_new_provider_api_key = "your_api_key"
text_new_provider_model_name = "model-1"
text_new_provider_base_url = "https://api.newprovider.com/v1"
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. é¦–å…ˆè¿è¡Œæµ‹è¯•è„šæœ¬æ£€æŸ¥é…ç½®
2. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯
3. æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥
4. å‚è€ƒæœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†

---

*æœ€åæ›´æ–°: 2025-01-07*
