# NarratoAI å¤§æ¨¡å‹æœåŠ¡è¿ç§»æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©å¼€å‘è€…å°†ç°æœ‰ä»£ç ä»æ—§çš„å¤§æ¨¡å‹è°ƒç”¨æ–¹å¼è¿ç§»åˆ°æ–°çš„ç»Ÿä¸€LLMæœåŠ¡æ¶æ„ã€‚æ–°æ¶æ„æä¾›äº†æ›´å¥½çš„æ¨¡å—åŒ–ã€é”™è¯¯å¤„ç†å’Œé…ç½®ç®¡ç†ã€‚

## ğŸ”„ è¿ç§»å¯¹æ¯”

### æ—§çš„è°ƒç”¨æ–¹å¼ vs æ–°çš„è°ƒç”¨æ–¹å¼

#### 1. è§†è§‰åˆ†æå™¨åˆ›å»º

**æ—§æ–¹å¼ï¼š**
```python
from app.utils import gemini_analyzer, qwenvl_analyzer

if provider == 'gemini':
    analyzer = gemini_analyzer.VisionAnalyzer(
        model_name=model, 
        api_key=api_key, 
        base_url=base_url
    )
elif provider == 'qwenvl':
    analyzer = qwenvl_analyzer.QwenAnalyzer(
        model_name=model,
        api_key=api_key,
        base_url=base_url
    )
```

**æ–°æ–¹å¼ï¼š**
```python
from app.services.llm.unified_service import UnifiedLLMService

# æ–¹å¼1: ç›´æ¥ä½¿ç”¨ç»Ÿä¸€æœåŠ¡
results = await UnifiedLLMService.analyze_images(
    images=images,
    prompt=prompt,
    provider=provider  # å¯é€‰ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
)

# æ–¹å¼2: ä½¿ç”¨è¿ç§»é€‚é…å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
from app.services.llm.migration_adapter import create_vision_analyzer
analyzer = create_vision_analyzer(provider, api_key, model, base_url)
results = await analyzer.analyze_images(images, prompt)
```

#### 2. æ–‡æœ¬ç”Ÿæˆ

**æ—§æ–¹å¼ï¼š**
```python
from openai import OpenAI

client = OpenAI(api_key=api_key, base_url=base_url)
response = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ],
    temperature=temperature,
    response_format={"type": "json_object"}
)
result = response.choices[0].message.content
```

**æ–°æ–¹å¼ï¼š**
```python
from app.services.llm.unified_service import UnifiedLLMService

result = await UnifiedLLMService.generate_text(
    prompt=prompt,
    system_prompt=system_prompt,
    temperature=temperature,
    response_format="json"
)
```

#### 3. è§£è¯´æ–‡æ¡ˆç”Ÿæˆ

**æ—§æ–¹å¼ï¼š**
```python
from app.services.generate_narration_script import generate_narration

narration = generate_narration(
    markdown_content,
    api_key,
    base_url=base_url,
    model=model
)
# æ‰‹åŠ¨è§£æJSONå’ŒéªŒè¯æ ¼å¼
import json
narration_dict = json.loads(narration)['items']
```

**æ–°æ–¹å¼ï¼š**
```python
from app.services.llm.unified_service import UnifiedLLMService

# è‡ªåŠ¨éªŒè¯è¾“å‡ºæ ¼å¼
narration_items = await UnifiedLLMService.generate_narration_script(
    prompt=prompt,
    validate_output=True  # è‡ªåŠ¨éªŒè¯JSONæ ¼å¼å’Œå­—æ®µ
)
```

## ğŸ“ å…·ä½“è¿ç§»æ­¥éª¤

### æ­¥éª¤1: æ›´æ–°é…ç½®æ–‡ä»¶

**æ—§é…ç½®æ ¼å¼ï¼š**
```toml
[app]
    llm_provider = "openai"
    openai_api_key = "sk-xxx"
    openai_model_name = "gpt-4"
    
    vision_llm_provider = "gemini"
    gemini_api_key = "xxx"
    gemini_model_name = "gemini-1.5-pro"
```

**æ–°é…ç½®æ ¼å¼ï¼š**
```toml
[app]
    # è§†è§‰æ¨¡å‹é…ç½®
    vision_llm_provider = "gemini"
    vision_gemini_api_key = "xxx"
    vision_gemini_model_name = "gemini-2.0-flash-lite"
    vision_gemini_base_url = "https://generativelanguage.googleapis.com/v1beta"
    
    # æ–‡æœ¬æ¨¡å‹é…ç½®
    text_llm_provider = "openai"
    text_openai_api_key = "sk-xxx"
    text_openai_model_name = "gpt-4o-mini"
    text_openai_base_url = "https://api.openai.com/v1"
```

### æ­¥éª¤2: æ›´æ–°å¯¼å…¥è¯­å¥

**æ—§å¯¼å…¥ï¼š**
```python
from app.utils import gemini_analyzer, qwenvl_analyzer
from app.services.generate_narration_script import generate_narration
from app.services.SDE.short_drama_explanation import analyze_subtitle
```

**æ–°å¯¼å…¥ï¼š**
```python
from app.services.llm.unified_service import UnifiedLLMService
from app.services.llm.migration_adapter import (
    create_vision_analyzer,
    SubtitleAnalyzerAdapter
)
```

### æ­¥éª¤3: æ›´æ–°å‡½æ•°è°ƒç”¨

#### å›¾ç‰‡åˆ†æè¿ç§»

**æ—§ä»£ç ï¼š**
```python
def analyze_images_old(provider, api_key, model, base_url, images, prompt):
    if provider == 'gemini':
        analyzer = gemini_analyzer.VisionAnalyzer(
            model_name=model, 
            api_key=api_key, 
            base_url=base_url
        )
    else:
        analyzer = qwenvl_analyzer.QwenAnalyzer(
            model_name=model,
            api_key=api_key,
            base_url=base_url
        )
    
    # åŒæ­¥è°ƒç”¨
    results = []
    for batch in batches:
        result = analyzer.analyze_batch(batch, prompt)
        results.append(result)
    return results
```

**æ–°ä»£ç ï¼š**
```python
async def analyze_images_new(images, prompt, provider=None):
    # å¼‚æ­¥è°ƒç”¨ï¼Œè‡ªåŠ¨æ‰¹å¤„ç†
    results = await UnifiedLLMService.analyze_images(
        images=images,
        prompt=prompt,
        provider=provider,
        batch_size=10
    )
    return results
```

#### å­—å¹•åˆ†æè¿ç§»

**æ—§ä»£ç ï¼š**
```python
from app.services.SDE.short_drama_explanation import analyze_subtitle

result = analyze_subtitle(
    subtitle_file_path=subtitle_path,
    api_key=api_key,
    model=model,
    base_url=base_url,
    provider=provider
)
```

**æ–°ä»£ç ï¼š**
```python
# æ–¹å¼1: ä½¿ç”¨ç»Ÿä¸€æœåŠ¡
with open(subtitle_path, 'r', encoding='utf-8') as f:
    subtitle_content = f.read()

result = await UnifiedLLMService.analyze_subtitle(
    subtitle_content=subtitle_content,
    provider=provider,
    validate_output=True
)

# æ–¹å¼2: ä½¿ç”¨é€‚é…å™¨
from app.services.llm.migration_adapter import SubtitleAnalyzerAdapter

analyzer = SubtitleAnalyzerAdapter(api_key, model, base_url, provider)
result = analyzer.analyze_subtitle(subtitle_content)
```

## ğŸ”§ å¸¸è§è¿ç§»é—®é¢˜

### 1. åŒæ­¥ vs å¼‚æ­¥è°ƒç”¨

**é—®é¢˜ï¼š** æ–°æ¶æ„ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼Œæ—§ä»£ç æ˜¯åŒæ­¥çš„ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# åœ¨åŒæ­¥å‡½æ•°ä¸­è°ƒç”¨å¼‚æ­¥å‡½æ•°
import asyncio

def sync_function():
    result = asyncio.run(UnifiedLLMService.generate_text(prompt))
    return result

# æˆ–è€…å°†æ•´ä¸ªå‡½æ•°æ”¹ä¸ºå¼‚æ­¥
async def async_function():
    result = await UnifiedLLMService.generate_text(prompt)
    return result
```

### 2. é…ç½®è·å–æ–¹å¼å˜åŒ–

**é—®é¢˜ï¼š** é…ç½®é”®åå‘ç”Ÿå˜åŒ–ã€‚

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# æ—§æ–¹å¼
api_key = config.app.get('openai_api_key')
model = config.app.get('openai_model_name')

# æ–°æ–¹å¼
provider = config.app.get('text_llm_provider', 'openai')
api_key = config.app.get(f'text_{provider}_api_key')
model = config.app.get(f'text_{provider}_model_name')
```

### 3. é”™è¯¯å¤„ç†æ›´æ–°

**æ—§æ–¹å¼ï¼š**
```python
try:
    result = some_llm_call()
except Exception as e:
    print(f"Error: {e}")
```

**æ–°æ–¹å¼ï¼š**
```python
from app.services.llm.exceptions import LLMServiceError, ValidationError

try:
    result = await UnifiedLLMService.generate_text(prompt)
except ValidationError as e:
    print(f"è¾“å‡ºéªŒè¯å¤±è´¥: {e.message}")
except LLMServiceError as e:
    print(f"LLMæœåŠ¡é”™è¯¯: {e.message}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

## âœ… è¿ç§»æ£€æŸ¥æ¸…å•

### é…ç½®è¿ç§»
- [ ] æ›´æ–°é…ç½®æ–‡ä»¶æ ¼å¼
- [ ] éªŒè¯æ‰€æœ‰APIå¯†é’¥é…ç½®æ­£ç¡®
- [ ] è¿è¡Œé…ç½®éªŒè¯å™¨æ£€æŸ¥

### ä»£ç è¿ç§»
- [ ] æ›´æ–°å¯¼å…¥è¯­å¥
- [ ] å°†åŒæ­¥è°ƒç”¨æ”¹ä¸ºå¼‚æ­¥è°ƒç”¨
- [ ] æ›´æ–°é”™è¯¯å¤„ç†æœºåˆ¶
- [ ] ä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¥å£

### æµ‹è¯•éªŒè¯
- [ ] è¿è¡ŒLLMæœåŠ¡æµ‹è¯•è„šæœ¬
- [ ] æµ‹è¯•æ‰€æœ‰åŠŸèƒ½æ¨¡å—
- [ ] éªŒè¯è¾“å‡ºæ ¼å¼æ­£ç¡®
- [ ] æ£€æŸ¥æ€§èƒ½å’Œç¨³å®šæ€§

### æ¸…ç†å·¥ä½œ
- [ ] ç§»é™¤æœªä½¿ç”¨çš„æ—§ä»£ç 
- [ ] æ›´æ–°æ–‡æ¡£å’Œæ³¨é‡Š
- [ ] æ¸…ç†è¿‡æ—¶çš„ä¾èµ–

## ğŸš€ è¿ç§»æœ€ä½³å®è·µ

### 1. æ¸è¿›å¼è¿ç§»
- å…ˆè¿ç§»ä¸€ä¸ªæ¨¡å—ï¼Œæµ‹è¯•é€šè¿‡åå†è¿ç§»å…¶ä»–æ¨¡å—
- ä¿ç•™æ—§ä»£ç ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
- ä½¿ç”¨è¿ç§»é€‚é…å™¨ç¡®ä¿å‘åå…¼å®¹

### 2. å……åˆ†æµ‹è¯•
- åœ¨æ¯ä¸ªè¿ç§»æ­¥éª¤åè¿è¡Œæµ‹è¯•
- æ¯”è¾ƒæ–°æ—§å®ç°çš„è¾“å‡ºç»“æœ
- æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†

### 3. ç›‘æ§å’Œæ—¥å¿—
- å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•
- ç›‘æ§APIè°ƒç”¨æˆåŠŸç‡
- è·Ÿè¸ªæ€§èƒ½æŒ‡æ ‡

### 4. æ–‡æ¡£æ›´æ–°
- æ›´æ–°ä»£ç æ³¨é‡Š
- æ›´æ–°APIæ–‡æ¡£
- è®°å½•è¿ç§»è¿‡ç¨‹ä¸­çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœåœ¨è¿ç§»è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. **æŸ¥çœ‹æµ‹è¯•è„šæœ¬è¾“å‡º**ï¼š
   ```bash
   python app/services/llm/test_llm_service.py
   ```

2. **éªŒè¯é…ç½®**ï¼š
   ```python
   from app.services.llm.config_validator import LLMConfigValidator
   results = LLMConfigValidator.validate_all_configs()
   LLMConfigValidator.print_validation_report(results)
   ```

3. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**ï¼š
   ```python
   from loguru import logger
   logger.add("migration.log", level="DEBUG")
   ```

4. **å‚è€ƒç¤ºä¾‹ä»£ç **ï¼š
   - æŸ¥çœ‹ `app/services/llm/test_llm_service.py` ä¸­çš„ä½¿ç”¨ç¤ºä¾‹
   - å‚è€ƒå·²è¿ç§»çš„æ–‡ä»¶å¦‚ `webui/tools/base.py`

---

*æœ€åæ›´æ–°: 2025-01-07*
